{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tribal-product",
   "metadata": {},
   "source": [
    "# Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-carpet",
   "metadata": {},
   "source": [
    "```bash\n",
    "├── project\n",
    "│   ├── models\n",
    "│   ├── script.ipynb\n",
    "├── test_dataset\n",
    "│   ├── BC\n",
    "│   ├── LT\n",
    "│   ├── test_data.csv\n",
    "└── train_dataset\n",
    "│   ├── BC\n",
    "│   ├── LT\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-closing",
   "metadata": {},
   "source": [
    "# OS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-first",
   "metadata": {},
   "source": [
    "- Ubuntu\n",
    "- Intel i9 10900\n",
    "- NVIDIA GeForce RTX 3080"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-tragedy",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lonely-india",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting parmap\n",
      "  Downloading parmap-1.5.3-py2.py3-none-any.whl (12 kB)\n",
      "Installing collected packages: parmap\n",
      "Successfully installed parmap-1.5.3\n",
      "Requirement already satisfied: timm in /opt/conda/lib/python3.7/site-packages (0.4.12)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm) (0.9.1+cu111)\n",
      "Requirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.7/site-packages (from timm) (1.8.1+cu111)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm) (1.19.5)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (8.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install parmap\n",
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "funky-oxford",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast\n",
    "from torchvision import models\n",
    "\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "import cv2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from itertools import permutations\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import timm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import parmap\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "stuffed-sequence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy verison : 1.19.5\n",
      "pandas version : 1.1.5\n",
      "opencv version : 4.5.2\n",
      "torch version : 1.8.1+cu111\n",
      "sklearn verison : 0.24.2\n",
      "albumentations version : 1.0.0\n"
     ]
    }
   ],
   "source": [
    "print('numpy verison :', np.__version__)\n",
    "print('pandas version :', pd.__version__)\n",
    "print('opencv version :', cv2.__version__)\n",
    "print('torch version :', torch.__version__)\n",
    "print('sklearn verison :', sklearn.__version__)\n",
    "print('albumentations version :', albu.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-proof",
   "metadata": {},
   "source": [
    "# Image Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "chinese-giving",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_crop(path):\n",
    "    data = Image.open(path)\n",
    "        \n",
    "    data = data - np.min(data)\n",
    "    data = data / np.max(data)\n",
    "    data = (data * 255).astype(np.uint8)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "attractive-stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(img, size):\n",
    "    im = Image.fromarray(img)\n",
    "    im = im.resize((size, size))\n",
    "    \n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "stock-rebate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(path):\n",
    "    size = 512\n",
    "    save_dir = os.path.join(path, \"resized\"+str(size))\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    f = []\n",
    "    for (_, _ , filenames) in os.walk(path):\n",
    "        f.extend(filenames)\n",
    "        \n",
    "    for filename in f:\n",
    "        img = read_crop(os.path.join(path, filename))\n",
    "        img = resize(img, size)\n",
    "        img.save(os.path.join(save_dir, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fatal-brown",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list = glob.glob(\"../train_dataset/BC/*\")\n",
    "path_list.extend(glob.glob(\"../train_dataset/LT/*\"))\n",
    "path_list.extend(glob.glob(\"../test_dataset/BC/*\"))\n",
    "path_list.extend(glob.glob(\"../test_dataset/LT/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ranging-details",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19934484c8454a8fa5cc2517d9d8f467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = parmap.map(save_image, path_list, pm_pbar=True, pm_processes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "competitive-newcastle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-installation",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-healing",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "1) Permutation\n",
    "\n",
    "- 각 식물 개체마다 순열로 정렬시켜 데이터 생성\n",
    "\n",
    "2) 보조 변수 생성\n",
    "\n",
    "- 모형 성능 향상을 위해 보조 변수 생성\n",
    "- 2가지의 보조변수 활용\n",
    "- 식물 개체의 종류(aux_input1 - CropDataset: LT / BC)\n",
    "- After와 Before 이미지 용량 차이(aux_input2 - CropDataset: numeric)\n",
    "- Delta에 영향을 주는 종속변수 생성(label1 - CropDataset) \\\n",
    "  After Date > Before Date이면 1 \\\n",
    "  After Date ≤ Before Date이면 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-earth",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "- Before와 After 이미지에 같은 종류의 Augmentation을 실시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "varied-library",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropDataset(Dataset):\n",
    "    def __init__(self, df, image_size, mode, f):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        assert mode in [\"train\", \"valid\"] \n",
    "        self.mode = mode\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            self.df = self.df.sample(frac=f, random_state=random_seed).reset_index(drop=True)\n",
    "            self.transform = albu.Compose([albu.Resize(self.image_size, self.image_size),\n",
    "                                            albu.RandomResizedCrop(height=self.image_size, width=self.image_size, \n",
    "                                                    scale=(0.25,1.0), ratio=(0.75, 1.3333333333333333), \n",
    "                                                    interpolation=1, p=1.0),\n",
    "                                            albu.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, \n",
    "                                                                  rotate_limit=30, interpolation=1, border_mode=0, value=0, p=0.25),\n",
    "                                            albu.HorizontalFlip(p=0.5),\n",
    "                                            albu.VerticalFlip(p=0.5),\n",
    "                                            albu.OneOf([\n",
    "                                                albu.MotionBlur(p=.2),\n",
    "                                                albu.MedianBlur(blur_limit=3, p=0.1),\n",
    "                                                albu.Blur(blur_limit=3, p=0.1),\n",
    "                                            ], p=0.25),\n",
    "                                            albu.OneOf([\n",
    "                                                albu.CLAHE(clip_limit=2),\n",
    "                                                albu.IAASharpen(),\n",
    "                                                albu.IAAEmboss(),\n",
    "                                                albu.RandomBrightnessContrast(),            \n",
    "                                            ], p=0.25),\n",
    "                                            albu.Cutout(num_holes=8, max_h_size=32, max_w_size=32, fill_value=0, p=0.25),\n",
    "                                            albu.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "                                            ToTensorV2(),\n",
    "                                        ], additional_targets={'image1': 'image'})\n",
    "        else:\n",
    "            self.transform = albu.Compose([\n",
    "                albu.Resize(self.image_size, self.image_size),\n",
    "                albu.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "                ToTensorV2(),\n",
    "            ], additional_targets={'image1': 'image'})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        before_img_path = self.df.loc[index, 'before_file_path']\n",
    "        after_img_path = self.df.loc[index, 'after_file_path']\n",
    "\n",
    "        before_img = cv2.imread(before_img_path)\n",
    "        after_img = cv2.imread(after_img_path)\n",
    "        \n",
    "        label1 = torch.FloatTensor([self.df.loc[index, \"label\"]])\n",
    "        label2 = torch.FloatTensor([self.df.loc[index, \"delta\"]])\n",
    "        aux_input1 = torch.FloatTensor([self.df.loc[index, \"cls\"]])\n",
    "\n",
    "        file_size1 = os.path.getsize(before_img_path)\n",
    "        file_size2 = os.path.getsize(after_img_path)\n",
    "        aux_input2 = torch.FloatTensor([file_size2/10000 - file_size1/10000])\n",
    "        \n",
    "        transformed = self.transform(image=before_img, image1=after_img)\n",
    "        \n",
    "        before_img = transformed['image'] \n",
    "        after_img = transformed['image1']\n",
    "               \n",
    "        return before_img, after_img, aux_input1, aux_input2, label1, label2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-demonstration",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-anaheim",
   "metadata": {},
   "source": [
    "- CNN2RNN 모형\n",
    "- 내부 CNN모형: ResNet50, ResNet101\n",
    "- 내부 RNN모형: Bidirectional RNN\n",
    "- fully connected layer의 input: RNN의 feature + 보조 변수(class, diff_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "friendly-bronze",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2RNN_Network(nn.Module):\n",
    "    def __init__(self, model, hidden_dim, img_size):\n",
    "        super(CNN2RNN_Network, self).__init__()\n",
    "        self.input_size = 2048\n",
    "            \n",
    "        if model == \"resnet50\":\n",
    "            model = models.resnet50(pretrained = True)\n",
    "        elif model == \"resnet101\":\n",
    "            model = models.resnet101(pretrained = True)\n",
    "        modules = list(model.children())[:-2]\n",
    "        self.feature_extract_model = nn.Sequential(*modules)\n",
    "        self.img_size = img_size\n",
    "        self.seq_length = 2\n",
    "\n",
    "        self.RNN = nn.RNN(input_size = self.input_size, hidden_size=hidden_dim, num_layers=2, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim*2 + 2, 1)\n",
    "        self.fc2 = nn.Linear(hidden_dim*2 + 3, 1)\n",
    "        \n",
    "    def forward(self,x, aux_input1, aux_input2):\n",
    "        x = x.view(-1,3,self.img_size, self.img_size)\n",
    "        x = self.feature_extract_model(x)\n",
    "        x = x.mean(dim=(-2,-1))\n",
    "        x = x.view(2,-1,self.input_size)\n",
    "        x,_ = self.RNN(x) \n",
    "\n",
    "        x = torch.cat([x[-1],aux_input1, aux_input2], dim=1)\n",
    "        prob = self.fc1(x)\n",
    "        x = torch.cat([x, prob], dim=1)\n",
    "        delta = self.fc2(x) \n",
    "        \n",
    "        return prob, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "inside-closer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batch_item, epoch, batch, training, scheduler):\n",
    "    before_img = batch_item[0].to(device)\n",
    "    after_img = batch_item[1].to(device)\n",
    "    aux_input1 = batch_item[2].to(device)\n",
    "    aux_input2 = batch_item[3].to(device)\n",
    "    label1 = batch_item[4].to(device)\n",
    "    label2 = batch_item[5].to(device)\n",
    "    \n",
    "    img = torch.stack([before_img, after_img])\n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    if training is True:\n",
    "        scheduler.step()\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(img, aux_input1, aux_input2)\n",
    "            loss1 = criterion1(output[0], label1)\n",
    "            loss2 = criterion2(output[1], label2)\n",
    "            loss = loss1 + loss2\n",
    "            predicted = torch.sigmoid(output[0].data) > 0.5\n",
    "            predicted = predicted.float()\n",
    "            total += label1.size(0)\n",
    "            correct += (predicted == label1).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.item(), loss2.item(), correct\n",
    "    else:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(img, aux_input1, aux_input2)\n",
    "            loss1 = criterion1(output[0], label1)\n",
    "            loss2 = criterion2(output[1], label2)\n",
    "            loss = loss1 + loss2\n",
    "            predicted = torch.sigmoid(output[0].data) > 0.5\n",
    "            predicted = predicted.float()\n",
    "            total += label1.size(0)\n",
    "            correct += (predicted == label1).sum().item()\n",
    "            \n",
    "        return loss.item(), loss2.item(), correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "liked-delicious",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= resnet101 =================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/albumentations/imgaug/transforms.py:222: FutureWarning: IAASharpen is deprecated. Please use Sharpen instead\n",
      "  warnings.warn(\"IAASharpen is deprecated. Please use Sharpen instead\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/albumentations/imgaug/transforms.py:165: FutureWarning: This augmentation is deprecated. Please use Emboss instead\n",
      "  warnings.warn(\"This augmentation is deprecated. Please use Emboss instead\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:645: FutureWarning: This class has been deprecated. Please use CoarseDropout\n",
      "  FutureWarning,\n",
      "  0%|          | 0/1022 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.22it/s, Epoch=1, LR=0.000594, Total Loss=2.898, Total Loss2=2.8037, Mean Accuracy=0.9595] \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.74it/s, Epoch=1, Total Val Loss=2.146, Total Val Loss2=2.0613, Mean Val Accuracy=0.9635] \n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.20it/s, Epoch=2, LR=3.5e-05, Total Loss=2.0416, Total Loss2=1.9828, Mean Accuracy=0.9757] \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.74it/s, Epoch=2, Total Val Loss=1.6938, Total Val Loss2=1.6521, Mean Val Accuracy=0.983] \n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.21it/s, Epoch=3, LR=0.000232, Total Loss=1.8433, Total Loss2=1.789, Mean Accuracy=0.9769] \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.74it/s, Epoch=3, Total Val Loss=1.4004, Total Val Loss2=1.3659, Mean Val Accuracy=0.9854]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.22it/s, Epoch=4, LR=0.000864, Total Loss=1.6391, Total Loss2=1.5937, Mean Accuracy=0.9812]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.75it/s, Epoch=4, Total Val Loss=1.5505, Total Val Loss2=1.505, Mean Val Accuracy=0.9851] \n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.23it/s, Epoch=5, LR=0.000905, Total Loss=1.5122, Total Loss2=1.4711, Mean Accuracy=0.9838]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.76it/s, Epoch=5, Total Val Loss=1.6232, Total Val Loss2=1.5758, Mean Val Accuracy=0.9813]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.24it/s, Epoch=6, LR=0.000287, Total Loss=1.4933, Total Loss2=1.4537, Mean Accuracy=0.9854]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.76it/s, Epoch=6, Total Val Loss=1.3967, Total Val Loss2=1.3645, Mean Val Accuracy=0.9897]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.21it/s, Epoch=7, LR=1.6e-05, Total Loss=1.4239, Total Loss2=1.3882, Mean Accuracy=0.9868] \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.74it/s, Epoch=7, Total Val Loss=1.2108, Total Val Loss2=1.1804, Mean Val Accuracy=0.9901]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.21it/s, Epoch=8, LR=0.000531, Total Loss=1.3298, Total Loss2=1.2939, Mean Accuracy=0.9862]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.74it/s, Epoch=8, Total Val Loss=1.0668, Total Val Loss2=1.04, Mean Val Accuracy=0.9901]  \n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.23it/s, Epoch=9, LR=0.000996, Total Loss=1.2774, Total Loss2=1.2473, Mean Accuracy=0.9889]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.74it/s, Epoch=9, Total Val Loss=1.4478, Total Val Loss2=1.4059, Mean Val Accuracy=0.9815]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.23it/s, Epoch=10, LR=0.000655, Total Loss=1.2297, Total Loss2=1.1958, Mean Accuracy=0.988] \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.75it/s, Epoch=10, Total Val Loss=1.2088, Total Val Loss2=1.1845, Mean Val Accuracy=0.9914]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.20it/s, Epoch=11, LR=6.2e-05, Total Loss=1.1829, Total Loss2=1.1499, Mean Accuracy=0.9894] \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.74it/s, Epoch=11, Total Val Loss=0.9907, Total Val Loss2=0.9633, Mean Val Accuracy=0.9926]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.21it/s, Epoch=12, LR=0.000181, Total Loss=1.1531, Total Loss2=1.1246, Mean Accuracy=0.9904]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.74it/s, Epoch=12, Total Val Loss=1.0381, Total Val Loss2=1.0106, Mean Val Accuracy=0.9928]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.20it/s, Epoch=13, LR=0.000819, Total Loss=1.1217, Total Loss2=1.0924, Mean Accuracy=0.9905]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.74it/s, Epoch=13, Total Val Loss=1.2235, Total Val Loss2=1.2, Mean Val Accuracy=0.9949]   \n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.22it/s, Epoch=14, LR=0.000938, Total Loss=1.0752, Total Loss2=1.0494, Mean Accuracy=0.991] \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.75it/s, Epoch=14, Total Val Loss=1.0711, Total Val Loss2=1.0405, Mean Val Accuracy=0.9875]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.22it/s, Epoch=15, LR=0.000345, Total Loss=1.0398, Total Loss2=1.0131, Mean Accuracy=0.9908]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.75it/s, Epoch=15, Total Val Loss=0.8716, Total Val Loss2=0.8472, Mean Val Accuracy=0.9933]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.21it/s, Epoch=16, LR=4e-06, Total Loss=1.0356, Total Loss2=1.0074, Mean Accuracy=0.9908]   \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.74it/s, Epoch=16, Total Val Loss=0.8561, Total Val Loss2=0.8342, Mean Val Accuracy=0.9933]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.23it/s, Epoch=17, LR=0.000469, Total Loss=0.9921, Total Loss2=0.9663, Mean Accuracy=0.991] \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.75it/s, Epoch=17, Total Val Loss=0.8008, Total Val Loss2=0.7788, Mean Val Accuracy=0.9932]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.21it/s, Epoch=18, LR=0.000984, Total Loss=0.9703, Total Loss2=0.9452, Mean Accuracy=0.9924]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.76it/s, Epoch=18, Total Val Loss=0.9289, Total Val Loss2=0.9106, Mean Val Accuracy=0.9947]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.24it/s, Epoch=19, LR=0.000713, Total Loss=0.9552, Total Loss2=0.9304, Mean Accuracy=0.9922]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.75it/s, Epoch=19, Total Val Loss=1.0302, Total Val Loss2=0.9918, Mean Val Accuracy=0.9803]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.22it/s, Epoch=20, LR=9.5e-05, Total Loss=0.9482, Total Loss2=0.925, Mean Accuracy=0.992]   \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.75it/s, Epoch=20, Total Val Loss=0.8063, Total Val Loss2=0.7821, Mean Val Accuracy=0.9937]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.22it/s, Epoch=21, LR=0.000136, Total Loss=0.931, Total Loss2=0.9084, Mean Accuracy=0.9928] \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.75it/s, Epoch=21, Total Val Loss=0.7859, Total Val Loss2=0.7664, Mean Val Accuracy=0.994] \n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.23it/s, Epoch=22, LR=0.000768, Total Loss=0.9111, Total Loss2=0.8879, Mean Accuracy=0.9928]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.75it/s, Epoch=22, Total Val Loss=0.8152, Total Val Loss2=0.7896, Mean Val Accuracy=0.9952]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.22it/s, Epoch=23, LR=0.000965, Total Loss=0.9179, Total Loss2=0.8947, Mean Accuracy=0.9935]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.75it/s, Epoch=23, Total Val Loss=0.9724, Total Val Loss2=0.9484, Mean Val Accuracy=0.9933]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.22it/s, Epoch=24, LR=0.000406, Total Loss=0.9063, Total Loss2=0.8832, Mean Accuracy=0.9925]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.75it/s, Epoch=24, Total Val Loss=0.7391, Total Val Loss2=0.7142, Mean Val Accuracy=0.995] \n",
      "100%|██████████| 1022/1022 [03:18<00:00,  5.14it/s, Epoch=25, LR=0.0, Total Loss=0.881, Total Loss2=0.8597, Mean Accuracy=0.9935]      \n",
      "100%|██████████| 365/365 [01:39<00:00,  3.66it/s, Epoch=25, Total Val Loss=0.7239, Total Val Loss2=0.7068, Mean Val Accuracy=0.9955]\n",
      "100%|██████████| 1022/1022 [03:23<00:00,  5.03it/s, Epoch=26, LR=0.000406, Total Loss=0.8588, Total Loss2=0.8363, Mean Accuracy=0.9927]\n",
      "100%|██████████| 365/365 [01:39<00:00,  3.66it/s, Epoch=26, Total Val Loss=0.6398, Total Val Loss2=0.6216, Mean Val Accuracy=0.9962]\n",
      "100%|██████████| 1022/1022 [03:19<00:00,  5.12it/s, Epoch=27, LR=0.000965, Total Loss=0.854, Total Loss2=0.8319, Mean Accuracy=0.9931] \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.75it/s, Epoch=27, Total Val Loss=0.7525, Total Val Loss2=0.7339, Mean Val Accuracy=0.9945]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.23it/s, Epoch=28, LR=0.000768, Total Loss=0.8558, Total Loss2=0.8329, Mean Accuracy=0.9924]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.75it/s, Epoch=28, Total Val Loss=0.7989, Total Val Loss2=0.7733, Mean Val Accuracy=0.9916]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.21it/s, Epoch=29, LR=0.000136, Total Loss=0.8455, Total Loss2=0.8235, Mean Accuracy=0.993] \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.73it/s, Epoch=29, Total Val Loss=0.7153, Total Val Loss2=0.6916, Mean Val Accuracy=0.9935]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.20it/s, Epoch=30, LR=9.5e-05, Total Loss=0.8045, Total Loss2=0.7868, Mean Accuracy=0.9954] \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.74it/s, Epoch=30, Total Val Loss=0.7245, Total Val Loss2=0.6954, Mean Val Accuracy=0.9901]\n",
      "/opt/conda/lib/python3.7/site-packages/albumentations/imgaug/transforms.py:222: FutureWarning: IAASharpen is deprecated. Please use Sharpen instead\n",
      "  warnings.warn(\"IAASharpen is deprecated. Please use Sharpen instead\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/albumentations/imgaug/transforms.py:165: FutureWarning: This augmentation is deprecated. Please use Emboss instead\n",
      "  warnings.warn(\"This augmentation is deprecated. Please use Emboss instead\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:645: FutureWarning: This class has been deprecated. Please use CoarseDropout\n",
      "  FutureWarning,\n",
      "  0%|          | 0/1022 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "100%|██████████| 1022/1022 [03:17<00:00,  5.19it/s, Epoch=1, LR=0.000594, Total Loss=2.9244, Total Loss2=2.8264, Mean Accuracy=0.9596]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.75it/s, Epoch=1, Total Val Loss=1.8294, Total Val Loss2=1.7626, Mean Val Accuracy=0.9767]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.20it/s, Epoch=2, LR=3.5e-05, Total Loss=2.13, Total Loss2=2.0615, Mean Accuracy=0.9724]   \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.75it/s, Epoch=2, Total Val Loss=1.7304, Total Val Loss2=1.6809, Mean Val Accuracy=0.9829]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.20it/s, Epoch=3, LR=0.000232, Total Loss=1.8134, Total Loss2=1.7605, Mean Accuracy=0.9798]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.76it/s, Epoch=3, Total Val Loss=1.4444, Total Val Loss2=1.4019, Mean Val Accuracy=0.9849]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.24it/s, Epoch=4, LR=0.000864, Total Loss=1.6218, Total Loss2=1.5772, Mean Accuracy=0.9851]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.76it/s, Epoch=4, Total Val Loss=1.4246, Total Val Loss2=1.3846, Mean Val Accuracy=0.9878]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.21it/s, Epoch=5, LR=0.000905, Total Loss=1.5237, Total Loss2=1.4824, Mean Accuracy=0.9845]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.75it/s, Epoch=5, Total Val Loss=1.5415, Total Val Loss2=1.505, Mean Val Accuracy=0.9861] \n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.20it/s, Epoch=6, LR=0.000287, Total Loss=1.4086, Total Loss2=1.3714, Mean Accuracy=0.9862]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.74it/s, Epoch=6, Total Val Loss=1.1658, Total Val Loss2=1.1394, Mean Val Accuracy=0.9923]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.21it/s, Epoch=7, LR=1.6e-05, Total Loss=1.3688, Total Loss2=1.3289, Mean Accuracy=0.9865] \n",
      "100%|██████████| 365/365 [01:36<00:00,  3.76it/s, Epoch=7, Total Val Loss=1.116, Total Val Loss2=1.0772, Mean Val Accuracy=0.9899] \n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.23it/s, Epoch=8, LR=0.000531, Total Loss=1.2978, Total Loss2=1.2617, Mean Accuracy=0.9876]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.76it/s, Epoch=8, Total Val Loss=1.0581, Total Val Loss2=1.0293, Mean Val Accuracy=0.992] \n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.23it/s, Epoch=9, LR=0.000996, Total Loss=1.2365, Total Loss2=1.205, Mean Accuracy=0.9892] \n",
      "100%|██████████| 365/365 [01:36<00:00,  3.77it/s, Epoch=9, Total Val Loss=1.2808, Total Val Loss2=1.2486, Mean Val Accuracy=0.9909]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.24it/s, Epoch=10, LR=0.000655, Total Loss=1.2022, Total Loss2=1.1719, Mean Accuracy=0.9899]\n",
      "100%|██████████| 365/365 [01:36<00:00,  3.77it/s, Epoch=10, Total Val Loss=1.1257, Total Val Loss2=1.1053, Mean Val Accuracy=0.9954]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.22it/s, Epoch=11, LR=6.2e-05, Total Loss=1.1349, Total Loss2=1.1063, Mean Accuracy=0.9908] \n",
      "100%|██████████| 365/365 [01:36<00:00,  3.76it/s, Epoch=11, Total Val Loss=0.9286, Total Val Loss2=0.9018, Mean Val Accuracy=0.989] \n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.22it/s, Epoch=12, LR=0.000181, Total Loss=1.1338, Total Loss2=1.1035, Mean Accuracy=0.9891]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.75it/s, Epoch=12, Total Val Loss=0.9273, Total Val Loss2=0.9042, Mean Val Accuracy=0.9937]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.22it/s, Epoch=13, LR=0.000819, Total Loss=1.1154, Total Loss2=1.0865, Mean Accuracy=0.9892]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.76it/s, Epoch=13, Total Val Loss=1.0654, Total Val Loss2=1.0429, Mean Val Accuracy=0.995] \n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.22it/s, Epoch=14, LR=0.000938, Total Loss=1.0649, Total Loss2=1.0361, Mean Accuracy=0.9903]\n",
      "100%|██████████| 365/365 [01:36<00:00,  3.77it/s, Epoch=14, Total Val Loss=1.0277, Total Val Loss2=1.0021, Mean Val Accuracy=0.9904]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.23it/s, Epoch=15, LR=0.000345, Total Loss=1.0319, Total Loss2=1.0067, Mean Accuracy=0.9923]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.76it/s, Epoch=15, Total Val Loss=1.0942, Total Val Loss2=1.0694, Mean Val Accuracy=0.9916]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.21it/s, Epoch=16, LR=4e-06, Total Loss=1.0317, Total Loss2=1.0051, Mean Accuracy=0.9908]   \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.74it/s, Epoch=16, Total Val Loss=0.8267, Total Val Loss2=0.8037, Mean Val Accuracy=0.9916]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.19it/s, Epoch=17, LR=0.000469, Total Loss=1.0044, Total Loss2=0.9783, Mean Accuracy=0.9919]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.75it/s, Epoch=17, Total Val Loss=0.8037, Total Val Loss2=0.783, Mean Val Accuracy=0.9935] \n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.21it/s, Epoch=18, LR=0.000984, Total Loss=1.0081, Total Loss2=0.9825, Mean Accuracy=0.992] \n",
      "100%|██████████| 365/365 [01:36<00:00,  3.76it/s, Epoch=18, Total Val Loss=0.9269, Total Val Loss2=0.9001, Mean Val Accuracy=0.989] \n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.23it/s, Epoch=19, LR=0.000713, Total Loss=0.9864, Total Loss2=0.9587, Mean Accuracy=0.9916]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.76it/s, Epoch=19, Total Val Loss=0.9115, Total Val Loss2=0.894, Mean Val Accuracy=0.9947] \n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.21it/s, Epoch=20, LR=9.5e-05, Total Loss=0.9551, Total Loss2=0.9296, Mean Accuracy=0.9923] \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.76it/s, Epoch=20, Total Val Loss=0.7979, Total Val Loss2=0.7809, Mean Val Accuracy=0.9955]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.19it/s, Epoch=21, LR=0.000136, Total Loss=0.9544, Total Loss2=0.9306, Mean Accuracy=0.9926]\n",
      "100%|██████████| 365/365 [01:38<00:00,  3.72it/s, Epoch=21, Total Val Loss=0.8691, Total Val Loss2=0.8438, Mean Val Accuracy=0.9937]\n",
      "100%|██████████| 1022/1022 [03:22<00:00,  5.04it/s, Epoch=22, LR=0.000768, Total Loss=0.9111, Total Loss2=0.8887, Mean Accuracy=0.9932]\n",
      "100%|██████████| 365/365 [01:39<00:00,  3.66it/s, Epoch=22, Total Val Loss=0.7705, Total Val Loss2=0.7506, Mean Val Accuracy=0.9949]\n",
      "100%|██████████| 1022/1022 [03:23<00:00,  5.01it/s, Epoch=23, LR=0.000965, Total Loss=0.9164, Total Loss2=0.8938, Mean Accuracy=0.9928]\n",
      "100%|██████████| 365/365 [01:38<00:00,  3.72it/s, Epoch=23, Total Val Loss=0.9313, Total Val Loss2=0.9059, Mean Val Accuracy=0.9909]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.24it/s, Epoch=24, LR=0.000406, Total Loss=0.9088, Total Loss2=0.8849, Mean Accuracy=0.9917]\n",
      "100%|██████████| 365/365 [01:36<00:00,  3.78it/s, Epoch=24, Total Val Loss=0.7656, Total Val Loss2=0.747, Mean Val Accuracy=0.9938] \n",
      "100%|██████████| 1022/1022 [03:14<00:00,  5.24it/s, Epoch=25, LR=0.0, Total Loss=0.9003, Total Loss2=0.8792, Mean Accuracy=0.9936]     \n",
      "100%|██████████| 365/365 [01:36<00:00,  3.77it/s, Epoch=25, Total Val Loss=0.7995, Total Val Loss2=0.7806, Mean Val Accuracy=0.9957]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.19it/s, Epoch=26, LR=0.000406, Total Loss=0.8679, Total Loss2=0.847, Mean Accuracy=0.9937] \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.75it/s, Epoch=26, Total Val Loss=0.6478, Total Val Loss2=0.6322, Mean Val Accuracy=0.9961]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.21it/s, Epoch=27, LR=0.000965, Total Loss=0.8667, Total Loss2=0.845, Mean Accuracy=0.9937] \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.75it/s, Epoch=27, Total Val Loss=0.8934, Total Val Loss2=0.8744, Mean Val Accuracy=0.9918]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.20it/s, Epoch=28, LR=0.000768, Total Loss=0.8782, Total Loss2=0.8558, Mean Accuracy=0.9925]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.75it/s, Epoch=28, Total Val Loss=1.0819, Total Val Loss2=1.0573, Mean Val Accuracy=0.9923]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.21it/s, Epoch=29, LR=0.000136, Total Loss=0.8643, Total Loss2=0.8445, Mean Accuracy=0.9944]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.76it/s, Epoch=29, Total Val Loss=0.7925, Total Val Loss2=0.7772, Mean Val Accuracy=0.9954]\n",
      "100%|██████████| 1022/1022 [03:14<00:00,  5.25it/s, Epoch=30, LR=9.5e-05, Total Loss=0.8341, Total Loss2=0.8134, Mean Accuracy=0.993]  \n",
      "100%|██████████| 365/365 [01:36<00:00,  3.77it/s, Epoch=30, Total Val Loss=0.7122, Total Val Loss2=0.6971, Mean Val Accuracy=0.9954]\n",
      "/opt/conda/lib/python3.7/site-packages/albumentations/imgaug/transforms.py:222: FutureWarning: IAASharpen is deprecated. Please use Sharpen instead\n",
      "  warnings.warn(\"IAASharpen is deprecated. Please use Sharpen instead\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/albumentations/imgaug/transforms.py:165: FutureWarning: This augmentation is deprecated. Please use Emboss instead\n",
      "  warnings.warn(\"This augmentation is deprecated. Please use Emboss instead\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:645: FutureWarning: This class has been deprecated. Please use CoarseDropout\n",
      "  FutureWarning,\n",
      "  0%|          | 0/1022 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.23it/s, Epoch=1, LR=0.000594, Total Loss=2.9346, Total Loss2=2.8367, Mean Accuracy=0.9588]\n",
      "100%|██████████| 365/365 [01:36<00:00,  3.76it/s, Epoch=1, Total Val Loss=2.6961, Total Val Loss2=2.61, Mean Val Accuracy=0.963]   \n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.21it/s, Epoch=2, LR=3.5e-05, Total Loss=2.0585, Total Loss2=2.0011, Mean Accuracy=0.9776] \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.76it/s, Epoch=2, Total Val Loss=1.8169, Total Val Loss2=1.7564, Mean Val Accuracy=0.981] \n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.23it/s, Epoch=3, LR=0.000232, Total Loss=1.852, Total Loss2=1.8014, Mean Accuracy=0.9804] \n",
      "100%|██████████| 365/365 [01:36<00:00,  3.77it/s, Epoch=3, Total Val Loss=1.3403, Total Val Loss2=1.2942, Mean Val Accuracy=0.9841]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.22it/s, Epoch=4, LR=0.000864, Total Loss=1.6745, Total Loss2=1.6287, Mean Accuracy=0.9826]\n",
      "100%|██████████| 365/365 [01:36<00:00,  3.78it/s, Epoch=4, Total Val Loss=1.3651, Total Val Loss2=1.3237, Mean Val Accuracy=0.9848]\n",
      "100%|██████████| 1022/1022 [03:14<00:00,  5.25it/s, Epoch=5, LR=0.000905, Total Loss=1.5543, Total Loss2=1.5135, Mean Accuracy=0.9843]\n",
      "100%|██████████| 365/365 [01:36<00:00,  3.78it/s, Epoch=5, Total Val Loss=1.4805, Total Val Loss2=1.4343, Mean Val Accuracy=0.9832]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.22it/s, Epoch=6, LR=0.000287, Total Loss=1.4484, Total Loss2=1.4127, Mean Accuracy=0.9872]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.75it/s, Epoch=6, Total Val Loss=1.2286, Total Val Loss2=1.1909, Mean Val Accuracy=0.9868]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.20it/s, Epoch=7, LR=1.6e-05, Total Loss=1.4003, Total Loss2=1.364, Mean Accuracy=0.9875]  \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.76it/s, Epoch=7, Total Val Loss=1.2273, Total Val Loss2=1.1953, Mean Val Accuracy=0.9872]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.23it/s, Epoch=8, LR=0.000531, Total Loss=1.3182, Total Loss2=1.2847, Mean Accuracy=0.9878]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.76it/s, Epoch=8, Total Val Loss=1.1047, Total Val Loss2=1.0703, Mean Val Accuracy=0.9892]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.22it/s, Epoch=9, LR=0.000996, Total Loss=1.2708, Total Loss2=1.2357, Mean Accuracy=0.9881]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.76it/s, Epoch=9, Total Val Loss=1.2438, Total Val Loss2=1.2094, Mean Val Accuracy=0.9904]\n",
      "100%|██████████| 1022/1022 [03:14<00:00,  5.25it/s, Epoch=10, LR=0.000655, Total Loss=1.2104, Total Loss2=1.179, Mean Accuracy=0.9892] \n",
      "100%|██████████| 365/365 [01:36<00:00,  3.78it/s, Epoch=10, Total Val Loss=1.1801, Total Val Loss2=1.1518, Mean Val Accuracy=0.9908]\n",
      "100%|██████████| 1022/1022 [03:14<00:00,  5.25it/s, Epoch=11, LR=6.2e-05, Total Loss=1.1987, Total Loss2=1.1668, Mean Accuracy=0.988]  \n",
      "100%|██████████| 365/365 [01:36<00:00,  3.79it/s, Epoch=11, Total Val Loss=1.0405, Total Val Loss2=1.0129, Mean Val Accuracy=0.9914]\n",
      "100%|██████████| 1022/1022 [03:14<00:00,  5.26it/s, Epoch=12, LR=0.000181, Total Loss=1.1419, Total Loss2=1.112, Mean Accuracy=0.9902] \n",
      "100%|██████████| 365/365 [01:36<00:00,  3.79it/s, Epoch=12, Total Val Loss=0.8487, Total Val Loss2=0.8241, Mean Val Accuracy=0.9952]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.22it/s, Epoch=13, LR=0.000819, Total Loss=1.099, Total Loss2=1.0726, Mean Accuracy=0.9908] \n",
      "100%|██████████| 365/365 [01:36<00:00,  3.79it/s, Epoch=13, Total Val Loss=0.9474, Total Val Loss2=0.9202, Mean Val Accuracy=0.9908]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.23it/s, Epoch=14, LR=0.000938, Total Loss=1.0624, Total Loss2=1.038, Mean Accuracy=0.9919] \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.75it/s, Epoch=14, Total Val Loss=1.1884, Total Val Loss2=1.1642, Mean Val Accuracy=0.9933]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.20it/s, Epoch=15, LR=0.000345, Total Loss=1.072, Total Loss2=1.0458, Mean Accuracy=0.9915] \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.76it/s, Epoch=15, Total Val Loss=0.9017, Total Val Loss2=0.8745, Mean Val Accuracy=0.9913]\n",
      "100%|██████████| 1022/1022 [03:19<00:00,  5.13it/s, Epoch=16, LR=4e-06, Total Loss=1.0031, Total Loss2=0.9785, Mean Accuracy=0.9919]   \n",
      "100%|██████████| 365/365 [01:39<00:00,  3.67it/s, Epoch=16, Total Val Loss=0.8439, Total Val Loss2=0.823, Mean Val Accuracy=0.9947] \n",
      "100%|██████████| 1022/1022 [03:22<00:00,  5.05it/s, Epoch=17, LR=0.000469, Total Loss=0.9947, Total Loss2=0.9711, Mean Accuracy=0.9925]\n",
      "100%|██████████| 365/365 [01:39<00:00,  3.67it/s, Epoch=17, Total Val Loss=0.8212, Total Val Loss2=0.8018, Mean Val Accuracy=0.9954]\n",
      "100%|██████████| 1022/1022 [03:17<00:00,  5.17it/s, Epoch=18, LR=0.000984, Total Loss=0.9749, Total Loss2=0.951, Mean Accuracy=0.9921] \n",
      "100%|██████████| 365/365 [01:36<00:00,  3.78it/s, Epoch=18, Total Val Loss=0.8758, Total Val Loss2=0.8529, Mean Val Accuracy=0.9928]\n",
      "100%|██████████| 1022/1022 [03:14<00:00,  5.25it/s, Epoch=19, LR=0.000713, Total Loss=0.9742, Total Loss2=0.9473, Mean Accuracy=0.9912]\n",
      "100%|██████████| 365/365 [01:36<00:00,  3.79it/s, Epoch=19, Total Val Loss=0.8594, Total Val Loss2=0.8334, Mean Val Accuracy=0.9916]\n",
      "100%|██████████| 1022/1022 [03:14<00:00,  5.25it/s, Epoch=20, LR=9.5e-05, Total Loss=0.9374, Total Loss2=0.9137, Mean Accuracy=0.9925] \n",
      "100%|██████████| 365/365 [01:36<00:00,  3.78it/s, Epoch=20, Total Val Loss=0.7847, Total Val Loss2=0.756, Mean Val Accuracy=0.9861] \n",
      "100%|██████████| 1022/1022 [03:14<00:00,  5.25it/s, Epoch=21, LR=0.000136, Total Loss=0.9156, Total Loss2=0.8924, Mean Accuracy=0.9926]\n",
      "100%|██████████| 365/365 [01:36<00:00,  3.76it/s, Epoch=21, Total Val Loss=0.7222, Total Val Loss2=0.7023, Mean Val Accuracy=0.9938]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.22it/s, Epoch=22, LR=0.000768, Total Loss=0.9299, Total Loss2=0.9069, Mean Accuracy=0.993] \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.76it/s, Epoch=22, Total Val Loss=0.8115, Total Val Loss2=0.7914, Mean Val Accuracy=0.9945]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.20it/s, Epoch=23, LR=0.000965, Total Loss=0.9146, Total Loss2=0.892, Mean Accuracy=0.9928] \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.76it/s, Epoch=23, Total Val Loss=1.1559, Total Val Loss2=1.1331, Mean Val Accuracy=0.9938]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.20it/s, Epoch=24, LR=0.000406, Total Loss=0.9039, Total Loss2=0.8807, Mean Accuracy=0.9933]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.75it/s, Epoch=24, Total Val Loss=0.8086, Total Val Loss2=0.7881, Mean Val Accuracy=0.9957]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.20it/s, Epoch=25, LR=0.0, Total Loss=0.8788, Total Loss2=0.8572, Mean Accuracy=0.9939]     \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.76it/s, Epoch=25, Total Val Loss=0.7581, Total Val Loss2=0.7374, Mean Val Accuracy=0.995] \n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.19it/s, Epoch=26, LR=0.000406, Total Loss=0.8844, Total Loss2=0.8644, Mean Accuracy=0.9946]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.76it/s, Epoch=26, Total Val Loss=0.8366, Total Val Loss2=0.8186, Mean Val Accuracy=0.9954]\n",
      "/opt/conda/lib/python3.7/site-packages/albumentations/imgaug/transforms.py:222: FutureWarning: IAASharpen is deprecated. Please use Sharpen instead\n",
      "  warnings.warn(\"IAASharpen is deprecated. Please use Sharpen instead\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/albumentations/imgaug/transforms.py:165: FutureWarning: This augmentation is deprecated. Please use Emboss instead\n",
      "  warnings.warn(\"This augmentation is deprecated. Please use Emboss instead\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:645: FutureWarning: This class has been deprecated. Please use CoarseDropout\n",
      "  FutureWarning,\n",
      "  0%|          | 0/1022 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "100%|██████████| 1022/1022 [03:17<00:00,  5.18it/s, Epoch=1, LR=0.000594, Total Loss=2.8829, Total Loss2=2.7904, Mean Accuracy=0.9617]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.76it/s, Epoch=1, Total Val Loss=2.3717, Total Val Loss2=2.3074, Mean Val Accuracy=0.9719]\n",
      "100%|██████████| 1022/1022 [03:17<00:00,  5.19it/s, Epoch=2, LR=3.5e-05, Total Loss=2.0776, Total Loss2=2.0182, Mean Accuracy=0.9761] \n",
      "100%|██████████| 365/365 [01:36<00:00,  3.77it/s, Epoch=2, Total Val Loss=1.7274, Total Val Loss2=1.6651, Mean Val Accuracy=0.9707]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.22it/s, Epoch=3, LR=0.000232, Total Loss=1.8021, Total Loss2=1.7507, Mean Accuracy=0.9798]\n",
      "100%|██████████| 365/365 [01:36<00:00,  3.78it/s, Epoch=3, Total Val Loss=1.5729, Total Val Loss2=1.5204, Mean Val Accuracy=0.9769]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.22it/s, Epoch=4, LR=0.000864, Total Loss=1.6679, Total Loss2=1.6233, Mean Accuracy=0.9826]\n",
      "100%|██████████| 365/365 [01:36<00:00,  3.78it/s, Epoch=4, Total Val Loss=1.4979, Total Val Loss2=1.4572, Mean Val Accuracy=0.987] \n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.22it/s, Epoch=5, LR=0.000905, Total Loss=1.5766, Total Loss2=1.5362, Mean Accuracy=0.9859]\n",
      "100%|██████████| 365/365 [01:36<00:00,  3.78it/s, Epoch=5, Total Val Loss=1.4883, Total Val Loss2=1.4382, Mean Val Accuracy=0.9784]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.22it/s, Epoch=6, LR=0.000287, Total Loss=1.4743, Total Loss2=1.4362, Mean Accuracy=0.9856]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.75it/s, Epoch=6, Total Val Loss=1.4926, Total Val Loss2=1.4565, Mean Val Accuracy=0.9872]\n",
      "100%|██████████| 1022/1022 [03:17<00:00,  5.18it/s, Epoch=7, LR=1.6e-05, Total Loss=1.3979, Total Loss2=1.3625, Mean Accuracy=0.987]  \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.76it/s, Epoch=7, Total Val Loss=1.2621, Total Val Loss2=1.2275, Mean Val Accuracy=0.988] \n",
      "100%|██████████| 1022/1022 [03:17<00:00,  5.18it/s, Epoch=8, LR=0.000531, Total Loss=1.3135, Total Loss2=1.2803, Mean Accuracy=0.989]\n",
      "100%|██████████| 365/365 [01:36<00:00,  3.77it/s, Epoch=8, Total Val Loss=1.0905, Total Val Loss2=1.0639, Mean Val Accuracy=0.9914]\n",
      "100%|██████████| 1022/1022 [03:17<00:00,  5.17it/s, Epoch=9, LR=0.000996, Total Loss=1.2859, Total Loss2=1.2564, Mean Accuracy=0.9898]\n",
      "100%|██████████| 365/365 [01:36<00:00,  3.77it/s, Epoch=9, Total Val Loss=1.2614, Total Val Loss2=1.2273, Mean Val Accuracy=0.9866]\n",
      "100%|██████████| 1022/1022 [03:17<00:00,  5.18it/s, Epoch=10, LR=0.000655, Total Loss=1.2229, Total Loss2=1.1937, Mean Accuracy=0.9906]\n",
      "100%|██████████| 365/365 [01:36<00:00,  3.77it/s, Epoch=10, Total Val Loss=1.3419, Total Val Loss2=1.306, Mean Val Accuracy=0.9854] \n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.21it/s, Epoch=11, LR=6.2e-05, Total Loss=1.2021, Total Loss2=1.1728, Mean Accuracy=0.9902] \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.76it/s, Epoch=11, Total Val Loss=1.1516, Total Val Loss2=1.1143, Mean Val Accuracy=0.9877]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.21it/s, Epoch=12, LR=0.000181, Total Loss=1.1648, Total Loss2=1.1377, Mean Accuracy=0.991]\n",
      "100%|██████████| 365/365 [01:36<00:00,  3.77it/s, Epoch=12, Total Val Loss=0.9061, Total Val Loss2=0.8786, Mean Val Accuracy=0.9902]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.19it/s, Epoch=13, LR=0.000819, Total Loss=1.1048, Total Loss2=1.0798, Mean Accuracy=0.9922]\n",
      "100%|██████████| 365/365 [01:36<00:00,  3.77it/s, Epoch=13, Total Val Loss=0.8937, Total Val Loss2=0.8698, Mean Val Accuracy=0.9935]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.21it/s, Epoch=14, LR=0.000938, Total Loss=1.1018, Total Loss2=1.0759, Mean Accuracy=0.9914]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.76it/s, Epoch=14, Total Val Loss=1.4939, Total Val Loss2=1.4653, Mean Val Accuracy=0.9887]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.20it/s, Epoch=15, LR=0.000345, Total Loss=1.037, Total Loss2=1.0137, Mean Accuracy=0.9927] \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.76it/s, Epoch=15, Total Val Loss=0.8603, Total Val Loss2=0.8404, Mean Val Accuracy=0.993] \n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.19it/s, Epoch=16, LR=4e-06, Total Loss=1.0441, Total Loss2=1.0177, Mean Accuracy=0.9909]   \n",
      "100%|██████████| 365/365 [01:36<00:00,  3.77it/s, Epoch=16, Total Val Loss=0.8409, Total Val Loss2=0.8157, Mean Val Accuracy=0.993] \n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.22it/s, Epoch=17, LR=0.000469, Total Loss=1.0216, Total Loss2=0.9969, Mean Accuracy=0.9919]\n",
      "100%|██████████| 365/365 [01:36<00:00,  3.77it/s, Epoch=17, Total Val Loss=0.9283, Total Val Loss2=0.9029, Mean Val Accuracy=0.9916]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.22it/s, Epoch=18, LR=0.000984, Total Loss=1.0075, Total Loss2=0.9836, Mean Accuracy=0.9911]\n",
      "100%|██████████| 365/365 [01:36<00:00,  3.77it/s, Epoch=18, Total Val Loss=0.9879, Total Val Loss2=0.9659, Mean Val Accuracy=0.9942]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.20it/s, Epoch=19, LR=0.000713, Total Loss=1.0022, Total Loss2=0.9791, Mean Accuracy=0.992] \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.76it/s, Epoch=19, Total Val Loss=0.9057, Total Val Loss2=0.8786, Mean Val Accuracy=0.9889]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.19it/s, Epoch=20, LR=9.5e-05, Total Loss=0.9665, Total Loss2=0.945, Mean Accuracy=0.9932]  \n",
      "100%|██████████| 365/365 [01:36<00:00,  3.77it/s, Epoch=20, Total Val Loss=0.7345, Total Val Loss2=0.7143, Mean Val Accuracy=0.9942]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.24it/s, Epoch=21, LR=0.000136, Total Loss=0.9543, Total Loss2=0.933, Mean Accuracy=0.9925] \n",
      "100%|██████████| 365/365 [01:36<00:00,  3.78it/s, Epoch=21, Total Val Loss=0.7937, Total Val Loss2=0.7709, Mean Val Accuracy=0.994] \n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.24it/s, Epoch=22, LR=0.000768, Total Loss=0.913, Total Loss2=0.8903, Mean Accuracy=0.9924] \n",
      "100%|██████████| 365/365 [01:36<00:00,  3.78it/s, Epoch=22, Total Val Loss=0.7888, Total Val Loss2=0.7677, Mean Val Accuracy=0.9928]\n",
      "100%|██████████| 1022/1022 [03:17<00:00,  5.19it/s, Epoch=23, LR=0.000965, Total Loss=0.91, Total Loss2=0.8911, Mean Accuracy=0.994]   \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.76it/s, Epoch=23, Total Val Loss=0.9269, Total Val Loss2=0.903, Mean Val Accuracy=0.9923] \n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.19it/s, Epoch=24, LR=0.000406, Total Loss=0.8908, Total Loss2=0.8719, Mean Accuracy=0.9939]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.75it/s, Epoch=24, Total Val Loss=0.8002, Total Val Loss2=0.7814, Mean Val Accuracy=0.9949]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.20it/s, Epoch=25, LR=0.0, Total Loss=0.9138, Total Loss2=0.8932, Mean Accuracy=0.9933]     \n",
      "100%|██████████| 365/365 [01:37<00:00,  3.76it/s, Epoch=25, Total Val Loss=0.7445, Total Val Loss2=0.7276, Mean Val Accuracy=0.9952]\n",
      "/opt/conda/lib/python3.7/site-packages/albumentations/imgaug/transforms.py:222: FutureWarning: IAASharpen is deprecated. Please use Sharpen instead\n",
      "  warnings.warn(\"IAASharpen is deprecated. Please use Sharpen instead\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/albumentations/imgaug/transforms.py:165: FutureWarning: This augmentation is deprecated. Please use Emboss instead\n",
      "  warnings.warn(\"This augmentation is deprecated. Please use Emboss instead\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:645: FutureWarning: This class has been deprecated. Please use CoarseDropout\n",
      "  FutureWarning,\n",
      "  0%|          | 0/1022 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.19it/s, Epoch=1, LR=0.000594, Total Loss=2.8891, Total Loss2=2.7973, Mean Accuracy=0.962] \n",
      "100%|██████████| 365/365 [01:36<00:00,  3.76it/s, Epoch=1, Total Val Loss=2.1856, Total Val Loss2=2.1165, Mean Val Accuracy=0.974] \n",
      "100%|██████████| 1022/1022 [03:17<00:00,  5.18it/s, Epoch=2, LR=3.5e-05, Total Loss=2.0113, Total Loss2=1.9497, Mean Accuracy=0.9755] \n",
      "100%|██████████| 365/365 [01:36<00:00,  3.77it/s, Epoch=2, Total Val Loss=1.6709, Total Val Loss2=1.6237, Mean Val Accuracy=0.9853]\n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.20it/s, Epoch=3, LR=0.000232, Total Loss=1.7954, Total Loss2=1.7463, Mean Accuracy=0.9805]\n",
      "100%|██████████| 365/365 [01:37<00:00,  3.76it/s, Epoch=3, Total Val Loss=1.425, Total Val Loss2=1.3868, Mean Val Accuracy=0.9856] \n",
      "100%|██████████| 1022/1022 [03:16<00:00,  5.19it/s, Epoch=4, LR=0.000864, Total Loss=1.6228, Total Loss2=1.577, Mean Accuracy=0.9819] \n",
      "100%|██████████| 365/365 [01:36<00:00,  3.78it/s, Epoch=4, Total Val Loss=1.3778, Total Val Loss2=1.3366, Mean Val Accuracy=0.9878]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.24it/s, Epoch=5, LR=0.000905, Total Loss=1.5206, Total Loss2=1.4802, Mean Accuracy=0.9854]\n",
      "100%|██████████| 365/365 [01:36<00:00,  3.78it/s, Epoch=5, Total Val Loss=1.8646, Total Val Loss2=1.8174, Mean Val Accuracy=0.98]  \n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.22it/s, Epoch=6, LR=0.000287, Total Loss=1.4749, Total Loss2=1.4364, Mean Accuracy=0.9858]\n",
      "100%|██████████| 365/365 [01:36<00:00,  3.77it/s, Epoch=6, Total Val Loss=1.4602, Total Val Loss2=1.4282, Mean Val Accuracy=0.9906]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.21it/s, Epoch=7, LR=1.6e-05, Total Loss=1.365, Total Loss2=1.3308, Mean Accuracy=0.9875]  \n",
      "100%|██████████| 365/365 [01:36<00:00,  3.77it/s, Epoch=7, Total Val Loss=1.1965, Total Val Loss2=1.1628, Mean Val Accuracy=0.9892]\n",
      "100%|██████████| 1022/1022 [03:17<00:00,  5.18it/s, Epoch=8, LR=0.000531, Total Loss=1.3442, Total Loss2=1.3083, Mean Accuracy=0.9875]\n",
      "100%|██████████| 365/365 [01:36<00:00,  3.78it/s, Epoch=8, Total Val Loss=1.1074, Total Val Loss2=1.0794, Mean Val Accuracy=0.9933]\n",
      "100%|██████████| 1022/1022 [03:14<00:00,  5.24it/s, Epoch=9, LR=0.000996, Total Loss=1.272, Total Loss2=1.24, Mean Accuracy=0.9886]   \n",
      "100%|██████████| 365/365 [01:36<00:00,  3.78it/s, Epoch=9, Total Val Loss=1.1957, Total Val Loss2=1.1573, Mean Val Accuracy=0.9851]\n",
      "100%|██████████| 1022/1022 [03:14<00:00,  5.26it/s, Epoch=10, LR=0.000655, Total Loss=1.219, Total Loss2=1.1884, Mean Accuracy=0.9892] \n",
      "100%|██████████| 365/365 [01:36<00:00,  3.79it/s, Epoch=10, Total Val Loss=1.2094, Total Val Loss2=1.1568, Mean Val Accuracy=0.9846]\n",
      "100%|██████████| 1022/1022 [03:14<00:00,  5.26it/s, Epoch=11, LR=6.2e-05, Total Loss=1.1616, Total Loss2=1.1326, Mean Accuracy=0.9904] \n",
      "100%|██████████| 365/365 [01:36<00:00,  3.79it/s, Epoch=11, Total Val Loss=1.0061, Total Val Loss2=0.9807, Mean Val Accuracy=0.9926]\n",
      "100%|██████████| 1022/1022 [03:19<00:00,  5.13it/s, Epoch=12, LR=0.000181, Total Loss=1.1363, Total Loss2=1.1099, Mean Accuracy=0.9908]\n",
      "100%|██████████| 365/365 [01:39<00:00,  3.69it/s, Epoch=12, Total Val Loss=0.9152, Total Val Loss2=0.891, Mean Val Accuracy=0.993]  \n",
      "100%|██████████| 1022/1022 [03:21<00:00,  5.07it/s, Epoch=13, LR=0.000819, Total Loss=1.109, Total Loss2=1.0817, Mean Accuracy=0.9908] \n",
      "100%|██████████| 365/365 [01:39<00:00,  3.68it/s, Epoch=13, Total Val Loss=0.8884, Total Val Loss2=0.8672, Mean Val Accuracy=0.9947]\n",
      "100%|██████████| 1022/1022 [03:21<00:00,  5.07it/s, Epoch=14, LR=0.000938, Total Loss=1.0841, Total Loss2=1.0571, Mean Accuracy=0.9903]\n",
      "100%|██████████| 365/365 [01:39<00:00,  3.68it/s, Epoch=14, Total Val Loss=1.1417, Total Val Loss2=1.1032, Mean Val Accuracy=0.9836]\n",
      "100%|██████████| 1022/1022 [03:21<00:00,  5.07it/s, Epoch=15, LR=0.000345, Total Loss=1.0456, Total Loss2=1.0185, Mean Accuracy=0.9914]\n",
      "100%|██████████| 365/365 [01:39<00:00,  3.68it/s, Epoch=15, Total Val Loss=1.0493, Total Val Loss2=1.0197, Mean Val Accuracy=0.9887]\n",
      "100%|██████████| 1022/1022 [03:17<00:00,  5.18it/s, Epoch=16, LR=4e-06, Total Loss=1.0258, Total Loss2=1.0024, Mean Accuracy=0.9932]   \n",
      "100%|██████████| 365/365 [01:36<00:00,  3.78it/s, Epoch=16, Total Val Loss=0.7822, Total Val Loss2=0.7638, Mean Val Accuracy=0.9955]\n",
      "100%|██████████| 1022/1022 [03:14<00:00,  5.24it/s, Epoch=17, LR=0.000469, Total Loss=1.0013, Total Loss2=0.9764, Mean Accuracy=0.9925]\n",
      "100%|██████████| 365/365 [01:36<00:00,  3.78it/s, Epoch=17, Total Val Loss=0.7309, Total Val Loss2=0.7109, Mean Val Accuracy=0.9942]\n",
      "100%|██████████| 1022/1022 [03:14<00:00,  5.25it/s, Epoch=18, LR=0.000984, Total Loss=0.9861, Total Loss2=0.9607, Mean Accuracy=0.9919]\n",
      "100%|██████████| 365/365 [01:36<00:00,  3.78it/s, Epoch=18, Total Val Loss=1.0011, Total Val Loss2=0.9825, Mean Val Accuracy=0.9955]\n",
      "100%|██████████| 1022/1022 [03:14<00:00,  5.25it/s, Epoch=19, LR=0.000713, Total Loss=0.9853, Total Loss2=0.9615, Mean Accuracy=0.993] \n",
      "100%|██████████| 365/365 [01:36<00:00,  3.78it/s, Epoch=19, Total Val Loss=0.9345, Total Val Loss2=0.9152, Mean Val Accuracy=0.9947]\n",
      "100%|██████████| 1022/1022 [03:14<00:00,  5.25it/s, Epoch=20, LR=9.5e-05, Total Loss=0.9607, Total Loss2=0.9381, Mean Accuracy=0.9927] \n",
      "100%|██████████| 365/365 [01:36<00:00,  3.78it/s, Epoch=20, Total Val Loss=0.8266, Total Val Loss2=0.8094, Mean Val Accuracy=0.9961]\n",
      "100%|██████████| 1022/1022 [03:15<00:00,  5.24it/s, Epoch=21, LR=0.000136, Total Loss=0.9254, Total Loss2=0.9028, Mean Accuracy=0.9932]\n",
      "100%|██████████| 365/365 [01:36<00:00,  3.78it/s, Epoch=21, Total Val Loss=0.7773, Total Val Loss2=0.7593, Mean Val Accuracy=0.995] \n",
      "100%|██████████| 1022/1022 [03:18<00:00,  5.16it/s, Epoch=22, LR=0.000768, Total Loss=0.9251, Total Loss2=0.9027, Mean Accuracy=0.9925]\n",
      "100%|██████████| 365/365 [01:39<00:00,  3.69it/s, Epoch=22, Total Val Loss=0.8055, Total Val Loss2=0.7863, Mean Val Accuracy=0.9945]\n",
      "/opt/conda/lib/python3.7/site-packages/albumentations/imgaug/transforms.py:222: FutureWarning: IAASharpen is deprecated. Please use Sharpen instead\n",
      "  warnings.warn(\"IAASharpen is deprecated. Please use Sharpen instead\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/albumentations/imgaug/transforms.py:165: FutureWarning: This augmentation is deprecated. Please use Emboss instead\n",
      "  warnings.warn(\"This augmentation is deprecated. Please use Emboss instead\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:645: FutureWarning: This class has been deprecated. Please use CoarseDropout\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= resnet50 =================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1022 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "100%|██████████| 1022/1022 [02:45<00:00,  6.16it/s, Epoch=1, LR=0.000594, Total Loss=2.9461, Total Loss2=2.8484, Mean Accuracy=0.9598]\n",
      "100%|██████████| 365/365 [01:01<00:00,  5.91it/s, Epoch=1, Total Val Loss=1.9148, Total Val Loss2=1.8524, Mean Val Accuracy=0.9777]\n",
      "100%|██████████| 1022/1022 [02:45<00:00,  6.16it/s, Epoch=2, LR=3.5e-05, Total Loss=2.0773, Total Loss2=2.0141, Mean Accuracy=0.9746] \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.01it/s, Epoch=2, Total Val Loss=1.8208, Total Val Loss2=1.7617, Mean Val Accuracy=0.9779]\n",
      "100%|██████████| 1022/1022 [02:36<00:00,  6.54it/s, Epoch=3, LR=0.000232, Total Loss=1.8939, Total Loss2=1.8406, Mean Accuracy=0.9794]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.07it/s, Epoch=3, Total Val Loss=1.4961, Total Val Loss2=1.45, Mean Val Accuracy=0.9851]  \n",
      "100%|██████████| 1022/1022 [02:36<00:00,  6.53it/s, Epoch=4, LR=0.000864, Total Loss=1.6901, Total Loss2=1.6404, Mean Accuracy=0.9811]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.05it/s, Epoch=4, Total Val Loss=1.5104, Total Val Loss2=1.4642, Mean Val Accuracy=0.9841]\n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.58it/s, Epoch=5, LR=0.000905, Total Loss=1.6027, Total Loss2=1.5594, Mean Accuracy=0.9835]\n",
      "100%|██████████| 365/365 [00:59<00:00,  6.09it/s, Epoch=5, Total Val Loss=4.1745, Total Val Loss2=4.0612, Mean Val Accuracy=0.9538]\n",
      "100%|██████████| 1022/1022 [02:33<00:00,  6.66it/s, Epoch=6, LR=0.000287, Total Loss=1.5361, Total Loss2=1.4954, Mean Accuracy=0.9856]\n",
      "100%|██████████| 365/365 [00:59<00:00,  6.10it/s, Epoch=6, Total Val Loss=1.3284, Total Val Loss2=1.2884, Mean Val Accuracy=0.9885]\n",
      "100%|██████████| 1022/1022 [02:33<00:00,  6.68it/s, Epoch=7, LR=1.6e-05, Total Loss=1.4298, Total Loss2=1.3921, Mean Accuracy=0.9859] \n",
      "100%|██████████| 365/365 [00:59<00:00,  6.10it/s, Epoch=7, Total Val Loss=1.2945, Total Val Loss2=1.2533, Mean Val Accuracy=0.9834]\n",
      "100%|██████████| 1022/1022 [02:33<00:00,  6.64it/s, Epoch=8, LR=0.000531, Total Loss=1.361, Total Loss2=1.326, Mean Accuracy=0.9872]  \n",
      "100%|██████████| 365/365 [00:59<00:00,  6.09it/s, Epoch=8, Total Val Loss=1.0526, Total Val Loss2=1.0254, Mean Val Accuracy=0.9925]\n",
      "100%|██████████| 1022/1022 [02:33<00:00,  6.64it/s, Epoch=9, LR=0.000996, Total Loss=1.2963, Total Loss2=1.264, Mean Accuracy=0.9887] \n",
      "100%|██████████| 365/365 [00:59<00:00,  6.08it/s, Epoch=9, Total Val Loss=1.66, Total Val Loss2=1.5988, Mean Val Accuracy=0.9728]  \n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.61it/s, Epoch=10, LR=0.000655, Total Loss=1.2552, Total Loss2=1.2194, Mean Accuracy=0.9878]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.06it/s, Epoch=10, Total Val Loss=1.3262, Total Val Loss2=1.3017, Mean Val Accuracy=0.9932]\n",
      "100%|██████████| 1022/1022 [02:44<00:00,  6.21it/s, Epoch=11, LR=6.2e-05, Total Loss=1.21, Total Loss2=1.1763, Mean Accuracy=0.9883]   \n",
      "100%|██████████| 365/365 [01:01<00:00,  5.89it/s, Epoch=11, Total Val Loss=1.1034, Total Val Loss2=1.0758, Mean Val Accuracy=0.9911]\n",
      "100%|██████████| 1022/1022 [02:45<00:00,  6.17it/s, Epoch=12, LR=0.000181, Total Loss=1.1501, Total Loss2=1.1171, Mean Accuracy=0.9897]\n",
      "100%|██████████| 365/365 [01:01<00:00,  5.91it/s, Epoch=12, Total Val Loss=0.9447, Total Val Loss2=0.9214, Mean Val Accuracy=0.9933]\n",
      "100%|██████████| 1022/1022 [02:44<00:00,  6.20it/s, Epoch=13, LR=0.000819, Total Loss=1.1309, Total Loss2=1.0987, Mean Accuracy=0.9888]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.07it/s, Epoch=13, Total Val Loss=1.0108, Total Val Loss2=0.9683, Mean Val Accuracy=0.9818]\n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.60it/s, Epoch=14, LR=0.000938, Total Loss=1.1018, Total Loss2=1.073, Mean Accuracy=0.9895] \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.07it/s, Epoch=14, Total Val Loss=1.0083, Total Val Loss2=0.9805, Mean Val Accuracy=0.9923]\n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.58it/s, Epoch=15, LR=0.000345, Total Loss=1.08, Total Loss2=1.0529, Mean Accuracy=0.9909]  \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.04it/s, Epoch=15, Total Val Loss=0.9751, Total Val Loss2=0.9486, Mean Val Accuracy=0.9935]\n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.61it/s, Epoch=16, LR=4e-06, Total Loss=1.0364, Total Loss2=1.0077, Mean Accuracy=0.9911]   \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.05it/s, Epoch=16, Total Val Loss=0.9076, Total Val Loss2=0.8822, Mean Val Accuracy=0.9908]\n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.59it/s, Epoch=17, LR=0.000469, Total Loss=1.0275, Total Loss2=0.9996, Mean Accuracy=0.9903]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.04it/s, Epoch=17, Total Val Loss=0.872, Total Val Loss2=0.8459, Mean Val Accuracy=0.9923] \n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.56it/s, Epoch=18, LR=0.000984, Total Loss=0.9968, Total Loss2=0.9696, Mean Accuracy=0.991] \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.04it/s, Epoch=18, Total Val Loss=0.9531, Total Val Loss2=0.9152, Mean Val Accuracy=0.9839]\n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.58it/s, Epoch=19, LR=0.000713, Total Loss=0.9757, Total Loss2=0.9516, Mean Accuracy=0.9921]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.04it/s, Epoch=19, Total Val Loss=1.3647, Total Val Loss2=1.3389, Mean Val Accuracy=0.9911]\n",
      "100%|██████████| 1022/1022 [02:36<00:00,  6.52it/s, Epoch=20, LR=9.5e-05, Total Loss=0.9671, Total Loss2=0.9414, Mean Accuracy=0.9919] \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.04it/s, Epoch=20, Total Val Loss=0.7665, Total Val Loss2=0.7419, Mean Val Accuracy=0.9911]\n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.59it/s, Epoch=21, LR=0.000136, Total Loss=0.9496, Total Loss2=0.9254, Mean Accuracy=0.993]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.04it/s, Epoch=21, Total Val Loss=0.7543, Total Val Loss2=0.7278, Mean Val Accuracy=0.9918]\n",
      "100%|██████████| 1022/1022 [02:36<00:00,  6.55it/s, Epoch=22, LR=0.000768, Total Loss=0.9359, Total Loss2=0.9128, Mean Accuracy=0.992] \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.05it/s, Epoch=22, Total Val Loss=0.6862, Total Val Loss2=0.6686, Mean Val Accuracy=0.9938]\n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.63it/s, Epoch=23, LR=0.000965, Total Loss=0.9014, Total Loss2=0.8817, Mean Accuracy=0.9943]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.04it/s, Epoch=23, Total Val Loss=0.7835, Total Val Loss2=0.7633, Mean Val Accuracy=0.9942]\n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.63it/s, Epoch=24, LR=0.000406, Total Loss=0.9119, Total Loss2=0.8908, Mean Accuracy=0.993] \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.03it/s, Epoch=24, Total Val Loss=0.8562, Total Val Loss2=0.8343, Mean Val Accuracy=0.9921]\n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.62it/s, Epoch=25, LR=0.0, Total Loss=0.8967, Total Loss2=0.8765, Mean Accuracy=0.9935]     \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.03it/s, Epoch=25, Total Val Loss=0.6747, Total Val Loss2=0.6521, Mean Val Accuracy=0.995] \n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.63it/s, Epoch=26, LR=0.000406, Total Loss=0.9077, Total Loss2=0.8875, Mean Accuracy=0.9941]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.03it/s, Epoch=26, Total Val Loss=0.8055, Total Val Loss2=0.7852, Mean Val Accuracy=0.9947]\n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.61it/s, Epoch=27, LR=0.000965, Total Loss=0.8836, Total Loss2=0.8624, Mean Accuracy=0.9936]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.01it/s, Epoch=27, Total Val Loss=0.8154, Total Val Loss2=0.7779, Mean Val Accuracy=0.9839]\n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.57it/s, Epoch=28, LR=0.000768, Total Loss=0.8695, Total Loss2=0.8483, Mean Accuracy=0.9922]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.00it/s, Epoch=28, Total Val Loss=0.8798, Total Val Loss2=0.8552, Mean Val Accuracy=0.9909]\n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.57it/s, Epoch=29, LR=0.000136, Total Loss=0.8311, Total Loss2=0.811, Mean Accuracy=0.9936] \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.00it/s, Epoch=29, Total Val Loss=0.7819, Total Val Loss2=0.7604, Mean Val Accuracy=0.9918]\n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.58it/s, Epoch=30, LR=9.5e-05, Total Loss=0.85, Total Loss2=0.8287, Mean Accuracy=0.9936]   \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.00it/s, Epoch=30, Total Val Loss=0.6554, Total Val Loss2=0.6388, Mean Val Accuracy=0.9954]\n",
      "/opt/conda/lib/python3.7/site-packages/albumentations/imgaug/transforms.py:222: FutureWarning: IAASharpen is deprecated. Please use Sharpen instead\n",
      "  warnings.warn(\"IAASharpen is deprecated. Please use Sharpen instead\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/albumentations/imgaug/transforms.py:165: FutureWarning: This augmentation is deprecated. Please use Emboss instead\n",
      "  warnings.warn(\"This augmentation is deprecated. Please use Emboss instead\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:645: FutureWarning: This class has been deprecated. Please use CoarseDropout\n",
      "  FutureWarning,\n",
      "  0%|          | 0/1022 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.58it/s, Epoch=1, LR=0.000594, Total Loss=2.9452, Total Loss2=2.8465, Mean Accuracy=0.9589]\n",
      "100%|██████████| 365/365 [01:00<00:00,  5.99it/s, Epoch=1, Total Val Loss=2.286, Total Val Loss2=2.1944, Mean Val Accuracy=0.9651] \n",
      "100%|██████████| 1022/1022 [02:36<00:00,  6.53it/s, Epoch=2, LR=3.5e-05, Total Loss=2.0981, Total Loss2=2.0382, Mean Accuracy=0.9763] \n",
      "100%|██████████| 365/365 [01:00<00:00,  5.99it/s, Epoch=2, Total Val Loss=1.6611, Total Val Loss2=1.6197, Mean Val Accuracy=0.9856]\n",
      "100%|██████████| 1022/1022 [02:37<00:00,  6.48it/s, Epoch=3, LR=0.000232, Total Loss=1.8055, Total Loss2=1.7528, Mean Accuracy=0.9785]\n",
      "100%|██████████| 365/365 [01:01<00:00,  5.96it/s, Epoch=3, Total Val Loss=1.4294, Total Val Loss2=1.3858, Mean Val Accuracy=0.9827]\n",
      "100%|██████████| 1022/1022 [02:36<00:00,  6.53it/s, Epoch=4, LR=0.000864, Total Loss=1.6723, Total Loss2=1.6285, Mean Accuracy=0.9832]\n",
      "100%|██████████| 365/365 [01:01<00:00,  5.95it/s, Epoch=4, Total Val Loss=1.3193, Total Val Loss2=1.2717, Mean Val Accuracy=0.9832]\n",
      "100%|██████████| 1022/1022 [02:36<00:00,  6.52it/s, Epoch=5, LR=0.000905, Total Loss=1.568, Total Loss2=1.5232, Mean Accuracy=0.982]  \n",
      "100%|██████████| 365/365 [01:01<00:00,  5.95it/s, Epoch=5, Total Val Loss=1.5508, Total Val Loss2=1.5132, Mean Val Accuracy=0.9849]\n",
      "100%|██████████| 1022/1022 [02:36<00:00,  6.53it/s, Epoch=6, LR=0.000287, Total Loss=1.4987, Total Loss2=1.4573, Mean Accuracy=0.9847]\n",
      "100%|██████████| 365/365 [01:01<00:00,  5.97it/s, Epoch=6, Total Val Loss=1.3155, Total Val Loss2=1.2815, Mean Val Accuracy=0.9865]\n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.56it/s, Epoch=7, LR=1.6e-05, Total Loss=1.4058, Total Loss2=1.3661, Mean Accuracy=0.9842] \n",
      "100%|██████████| 365/365 [01:01<00:00,  5.97it/s, Epoch=7, Total Val Loss=1.247, Total Val Loss2=1.2195, Mean Val Accuracy=0.9925] \n",
      "100%|██████████| 1022/1022 [02:36<00:00,  6.55it/s, Epoch=8, LR=0.000531, Total Loss=1.3714, Total Loss2=1.3354, Mean Accuracy=0.9865]\n",
      "100%|██████████| 365/365 [01:01<00:00,  5.93it/s, Epoch=8, Total Val Loss=1.124, Total Val Loss2=1.0964, Mean Val Accuracy=0.992]  \n",
      "100%|██████████| 1022/1022 [02:38<00:00,  6.47it/s, Epoch=9, LR=0.000996, Total Loss=1.3, Total Loss2=1.2639, Mean Accuracy=0.9864]   \n",
      "100%|██████████| 365/365 [01:01<00:00,  5.96it/s, Epoch=9, Total Val Loss=1.171, Total Val Loss2=1.133, Mean Val Accuracy=0.9834]  \n",
      "100%|██████████| 1022/1022 [02:37<00:00,  6.50it/s, Epoch=10, LR=0.000655, Total Loss=1.2654, Total Loss2=1.2294, Mean Accuracy=0.9878]\n",
      "100%|██████████| 365/365 [01:01<00:00,  5.95it/s, Epoch=10, Total Val Loss=1.3034, Total Val Loss2=1.25, Mean Val Accuracy=0.9791]  \n",
      "100%|██████████| 1022/1022 [02:36<00:00,  6.53it/s, Epoch=11, LR=6.2e-05, Total Loss=1.2101, Total Loss2=1.1779, Mean Accuracy=0.9887] \n",
      "100%|██████████| 365/365 [01:01<00:00,  5.97it/s, Epoch=11, Total Val Loss=0.9239, Total Val Loss2=0.8975, Mean Val Accuracy=0.995] \n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.58it/s, Epoch=12, LR=0.000181, Total Loss=1.1747, Total Loss2=1.1402, Mean Accuracy=0.988] \n",
      "100%|██████████| 365/365 [01:01<00:00,  5.97it/s, Epoch=12, Total Val Loss=0.9173, Total Val Loss2=0.8974, Mean Val Accuracy=0.9954]\n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.59it/s, Epoch=13, LR=0.000819, Total Loss=1.1243, Total Loss2=1.0973, Mean Accuracy=0.99]  \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.01it/s, Epoch=13, Total Val Loss=0.8514, Total Val Loss2=0.8315, Mean Val Accuracy=0.9949]\n",
      "100%|██████████| 1022/1022 [02:36<00:00,  6.53it/s, Epoch=14, LR=0.000938, Total Loss=1.1038, Total Loss2=1.0768, Mean Accuracy=0.9912]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.01it/s, Epoch=14, Total Val Loss=0.894, Total Val Loss2=0.8728, Mean Val Accuracy=0.9932] \n",
      "100%|██████████| 1022/1022 [02:37<00:00,  6.48it/s, Epoch=15, LR=0.000345, Total Loss=1.0789, Total Loss2=1.0507, Mean Accuracy=0.9903]\n",
      "100%|██████████| 365/365 [01:01<00:00,  5.98it/s, Epoch=15, Total Val Loss=0.8921, Total Val Loss2=0.867, Mean Val Accuracy=0.9906] \n",
      "100%|██████████| 1022/1022 [02:38<00:00,  6.46it/s, Epoch=16, LR=4e-06, Total Loss=1.0773, Total Loss2=1.0507, Mean Accuracy=0.9917]   \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.04it/s, Epoch=16, Total Val Loss=0.7974, Total Val Loss2=0.7837, Mean Val Accuracy=0.9976]\n",
      "100%|██████████| 1022/1022 [02:33<00:00,  6.64it/s, Epoch=17, LR=0.000469, Total Loss=1.0085, Total Loss2=0.9841, Mean Accuracy=0.9917]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.05it/s, Epoch=17, Total Val Loss=0.7788, Total Val Loss2=0.7626, Mean Val Accuracy=0.9964]\n",
      "100%|██████████| 1022/1022 [02:33<00:00,  6.66it/s, Epoch=18, LR=0.000984, Total Loss=1.0316, Total Loss2=1.0067, Mean Accuracy=0.9916]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.08it/s, Epoch=18, Total Val Loss=0.9558, Total Val Loss2=0.9317, Mean Val Accuracy=0.9918]\n",
      "100%|██████████| 1022/1022 [02:33<00:00,  6.67it/s, Epoch=19, LR=0.000713, Total Loss=0.979, Total Loss2=0.955, Mean Accuracy=0.9921]  \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.08it/s, Epoch=19, Total Val Loss=1.0502, Total Val Loss2=1.0266, Mean Val Accuracy=0.9913]\n",
      "100%|██████████| 1022/1022 [02:33<00:00,  6.65it/s, Epoch=20, LR=9.5e-05, Total Loss=0.9981, Total Loss2=0.9721, Mean Accuracy=0.9917] \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.06it/s, Epoch=20, Total Val Loss=0.8533, Total Val Loss2=0.8284, Mean Val Accuracy=0.9916]\n",
      "100%|██████████| 1022/1022 [02:33<00:00,  6.65it/s, Epoch=21, LR=0.000136, Total Loss=0.9606, Total Loss2=0.9361, Mean Accuracy=0.9926]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.05it/s, Epoch=21, Total Val Loss=0.8522, Total Val Loss2=0.8345, Mean Val Accuracy=0.9955]\n",
      "100%|██████████| 1022/1022 [02:33<00:00,  6.65it/s, Epoch=22, LR=0.000768, Total Loss=0.9512, Total Loss2=0.9257, Mean Accuracy=0.9915]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.04it/s, Epoch=22, Total Val Loss=0.8426, Total Val Loss2=0.8267, Mean Val Accuracy=0.9955]\n",
      "/opt/conda/lib/python3.7/site-packages/albumentations/imgaug/transforms.py:222: FutureWarning: IAASharpen is deprecated. Please use Sharpen instead\n",
      "  warnings.warn(\"IAASharpen is deprecated. Please use Sharpen instead\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/albumentations/imgaug/transforms.py:165: FutureWarning: This augmentation is deprecated. Please use Emboss instead\n",
      "  warnings.warn(\"This augmentation is deprecated. Please use Emboss instead\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:645: FutureWarning: This class has been deprecated. Please use CoarseDropout\n",
      "  FutureWarning,\n",
      "  0%|          | 0/1022 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.63it/s, Epoch=1, LR=0.000594, Total Loss=2.8559, Total Loss2=2.7634, Mean Accuracy=0.9614]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.03it/s, Epoch=1, Total Val Loss=2.0245, Total Val Loss2=1.9501, Mean Val Accuracy=0.9627]\n",
      "100%|██████████| 1022/1022 [02:33<00:00,  6.64it/s, Epoch=2, LR=3.5e-05, Total Loss=2.0779, Total Loss2=2.0165, Mean Accuracy=0.9747] \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.04it/s, Epoch=2, Total Val Loss=1.7246, Total Val Loss2=1.6674, Mean Val Accuracy=0.9772]\n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.56it/s, Epoch=3, LR=0.000232, Total Loss=1.8335, Total Loss2=1.7848, Mean Accuracy=0.9813]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.01it/s, Epoch=3, Total Val Loss=1.3648, Total Val Loss2=1.3241, Mean Val Accuracy=0.9889]\n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.55it/s, Epoch=4, LR=0.000864, Total Loss=1.7158, Total Loss2=1.6697, Mean Accuracy=0.9826]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.02it/s, Epoch=4, Total Val Loss=1.7533, Total Val Loss2=1.7115, Mean Val Accuracy=0.9885]\n",
      "100%|██████████| 1022/1022 [02:37<00:00,  6.48it/s, Epoch=5, LR=0.000905, Total Loss=1.6076, Total Loss2=1.5641, Mean Accuracy=0.9831]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.01it/s, Epoch=5, Total Val Loss=1.7283, Total Val Loss2=1.6864, Mean Val Accuracy=0.9839]\n",
      "100%|██████████| 1022/1022 [02:37<00:00,  6.49it/s, Epoch=6, LR=0.000287, Total Loss=1.5155, Total Loss2=1.4747, Mean Accuracy=0.985] \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.00it/s, Epoch=6, Total Val Loss=1.313, Total Val Loss2=1.2735, Mean Val Accuracy=0.9848] \n",
      "100%|██████████| 1022/1022 [02:38<00:00,  6.45it/s, Epoch=7, LR=1.6e-05, Total Loss=1.4199, Total Loss2=1.3827, Mean Accuracy=0.9869] \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.00it/s, Epoch=7, Total Val Loss=1.1611, Total Val Loss2=1.1083, Mean Val Accuracy=0.9789]\n",
      "100%|██████████| 1022/1022 [02:37<00:00,  6.48it/s, Epoch=8, LR=0.000531, Total Loss=1.3664, Total Loss2=1.331, Mean Accuracy=0.9873] \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.04it/s, Epoch=8, Total Val Loss=1.1172, Total Val Loss2=1.0821, Mean Val Accuracy=0.9884]\n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.63it/s, Epoch=9, LR=0.000996, Total Loss=1.313, Total Loss2=1.279, Mean Accuracy=0.9876]  \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.04it/s, Epoch=9, Total Val Loss=1.2805, Total Val Loss2=1.252, Mean Val Accuracy=0.9902] \n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.60it/s, Epoch=10, LR=0.000655, Total Loss=1.2636, Total Loss2=1.231, Mean Accuracy=0.9886] \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.04it/s, Epoch=10, Total Val Loss=1.5633, Total Val Loss2=1.5157, Mean Val Accuracy=0.9856]\n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.61it/s, Epoch=11, LR=6.2e-05, Total Loss=1.222, Total Loss2=1.1915, Mean Accuracy=0.99]    \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.03it/s, Epoch=11, Total Val Loss=0.9458, Total Val Loss2=0.9145, Mean Val Accuracy=0.9892]\n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.55it/s, Epoch=12, LR=0.000181, Total Loss=1.1955, Total Loss2=1.1658, Mean Accuracy=0.9906]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.02it/s, Epoch=12, Total Val Loss=1.0457, Total Val Loss2=1.011, Mean Val Accuracy=0.9921] \n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.56it/s, Epoch=13, LR=0.000819, Total Loss=1.1394, Total Loss2=1.1114, Mean Accuracy=0.9905]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.04it/s, Epoch=13, Total Val Loss=1.1835, Total Val Loss2=1.153, Mean Val Accuracy=0.988]  \n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.61it/s, Epoch=14, LR=0.000938, Total Loss=1.0837, Total Loss2=1.0566, Mean Accuracy=0.9916]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.04it/s, Epoch=14, Total Val Loss=1.1208, Total Val Loss2=1.0842, Mean Val Accuracy=0.9872]\n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.60it/s, Epoch=15, LR=0.000345, Total Loss=1.0753, Total Loss2=1.0496, Mean Accuracy=0.9914]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.04it/s, Epoch=15, Total Val Loss=0.9455, Total Val Loss2=0.9167, Mean Val Accuracy=0.9923]\n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.60it/s, Epoch=16, LR=4e-06, Total Loss=1.0362, Total Loss2=1.0106, Mean Accuracy=0.9913]   \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.05it/s, Epoch=16, Total Val Loss=0.7985, Total Val Loss2=0.7764, Mean Val Accuracy=0.9945]\n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.60it/s, Epoch=17, LR=0.000469, Total Loss=1.0339, Total Loss2=1.0071, Mean Accuracy=0.991] \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.04it/s, Epoch=17, Total Val Loss=0.762, Total Val Loss2=0.7385, Mean Val Accuracy=0.994]  \n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.62it/s, Epoch=18, LR=0.000984, Total Loss=1.0212, Total Loss2=0.9932, Mean Accuracy=0.9911]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.03it/s, Epoch=18, Total Val Loss=0.9115, Total Val Loss2=0.886, Mean Val Accuracy=0.9932] \n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.64it/s, Epoch=19, LR=0.000713, Total Loss=0.9832, Total Loss2=0.9597, Mean Accuracy=0.9927]\n",
      "100%|██████████| 365/365 [01:01<00:00,  5.93it/s, Epoch=19, Total Val Loss=0.8678, Total Val Loss2=0.8468, Mean Val Accuracy=0.9926]\n",
      "100%|██████████| 1022/1022 [02:46<00:00,  6.13it/s, Epoch=20, LR=9.5e-05, Total Loss=0.9678, Total Loss2=0.9436, Mean Accuracy=0.9927] \n",
      "100%|██████████| 365/365 [01:02<00:00,  5.85it/s, Epoch=20, Total Val Loss=0.7895, Total Val Loss2=0.7691, Mean Val Accuracy=0.9945]\n",
      "100%|██████████| 1022/1022 [02:46<00:00,  6.15it/s, Epoch=21, LR=0.000136, Total Loss=0.9556, Total Loss2=0.9313, Mean Accuracy=0.9921]\n",
      "100%|██████████| 365/365 [01:02<00:00,  5.85it/s, Epoch=21, Total Val Loss=0.8086, Total Val Loss2=0.7858, Mean Val Accuracy=0.9949]\n",
      "100%|██████████| 1022/1022 [02:41<00:00,  6.33it/s, Epoch=22, LR=0.000768, Total Loss=0.9679, Total Loss2=0.9422, Mean Accuracy=0.9916]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.05it/s, Epoch=22, Total Val Loss=0.8668, Total Val Loss2=0.8437, Mean Val Accuracy=0.9925]\n",
      "/opt/conda/lib/python3.7/site-packages/albumentations/imgaug/transforms.py:222: FutureWarning: IAASharpen is deprecated. Please use Sharpen instead\n",
      "  warnings.warn(\"IAASharpen is deprecated. Please use Sharpen instead\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/albumentations/imgaug/transforms.py:165: FutureWarning: This augmentation is deprecated. Please use Emboss instead\n",
      "  warnings.warn(\"This augmentation is deprecated. Please use Emboss instead\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:645: FutureWarning: This class has been deprecated. Please use CoarseDropout\n",
      "  FutureWarning,\n",
      "  0%|          | 0/1022 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.63it/s, Epoch=1, LR=0.000594, Total Loss=2.9191, Total Loss2=2.8226, Mean Accuracy=0.9592]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.05it/s, Epoch=1, Total Val Loss=2.9641, Total Val Loss2=2.9001, Mean Val Accuracy=0.9736]\n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.63it/s, Epoch=2, LR=3.5e-05, Total Loss=2.0599, Total Loss2=1.999, Mean Accuracy=0.9756]  \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.06it/s, Epoch=2, Total Val Loss=1.7187, Total Val Loss2=1.6624, Mean Val Accuracy=0.98]  \n",
      "100%|██████████| 1022/1022 [02:43<00:00,  6.25it/s, Epoch=3, LR=0.000232, Total Loss=1.7943, Total Loss2=1.7444, Mean Accuracy=0.9811]\n",
      "100%|██████████| 365/365 [01:02<00:00,  5.86it/s, Epoch=3, Total Val Loss=1.3281, Total Val Loss2=1.2863, Mean Val Accuracy=0.9865]\n",
      "100%|██████████| 1022/1022 [02:46<00:00,  6.14it/s, Epoch=4, LR=0.000864, Total Loss=1.6945, Total Loss2=1.6504, Mean Accuracy=0.9831]\n",
      "100%|██████████| 365/365 [01:02<00:00,  5.85it/s, Epoch=4, Total Val Loss=1.5204, Total Val Loss2=1.4574, Mean Val Accuracy=0.969] \n",
      "100%|██████████| 1022/1022 [02:46<00:00,  6.14it/s, Epoch=5, LR=0.000905, Total Loss=1.6002, Total Loss2=1.5563, Mean Accuracy=0.9836]\n",
      "100%|██████████| 365/365 [01:01<00:00,  5.97it/s, Epoch=5, Total Val Loss=1.6593, Total Val Loss2=1.6061, Mean Val Accuracy=0.9743]\n",
      "100%|██████████| 1022/1022 [02:33<00:00,  6.64it/s, Epoch=6, LR=0.000287, Total Loss=1.5223, Total Loss2=1.4818, Mean Accuracy=0.9848]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.06it/s, Epoch=6, Total Val Loss=1.213, Total Val Loss2=1.1736, Mean Val Accuracy=0.9887] \n",
      "100%|██████████| 1022/1022 [02:33<00:00,  6.65it/s, Epoch=7, LR=1.6e-05, Total Loss=1.4301, Total Loss2=1.3927, Mean Accuracy=0.9866] \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.06it/s, Epoch=7, Total Val Loss=1.1905, Total Val Loss2=1.1418, Mean Val Accuracy=0.9805]\n",
      "100%|██████████| 1022/1022 [02:36<00:00,  6.53it/s, Epoch=8, LR=0.000531, Total Loss=1.3635, Total Loss2=1.3281, Mean Accuracy=0.9879]\n",
      "100%|██████████| 365/365 [01:02<00:00,  5.87it/s, Epoch=8, Total Val Loss=0.999, Total Val Loss2=0.9647, Mean Val Accuracy=0.987]  \n",
      "100%|██████████| 1022/1022 [02:46<00:00,  6.15it/s, Epoch=9, LR=0.000996, Total Loss=1.3346, Total Loss2=1.3005, Mean Accuracy=0.9881]\n",
      "100%|██████████| 365/365 [01:02<00:00,  5.86it/s, Epoch=9, Total Val Loss=1.468, Total Val Loss2=1.4058, Mean Val Accuracy=0.9724] \n",
      "100%|██████████| 1022/1022 [02:46<00:00,  6.14it/s, Epoch=10, LR=0.000655, Total Loss=1.2732, Total Loss2=1.2394, Mean Accuracy=0.9881]\n",
      "100%|██████████| 365/365 [01:02<00:00,  5.85it/s, Epoch=10, Total Val Loss=1.7213, Total Val Loss2=1.6538, Mean Val Accuracy=0.9676]\n",
      "100%|██████████| 1022/1022 [02:38<00:00,  6.45it/s, Epoch=11, LR=6.2e-05, Total Loss=1.2137, Total Loss2=1.1832, Mean Accuracy=0.9899] \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.05it/s, Epoch=11, Total Val Loss=1.0141, Total Val Loss2=0.9787, Mean Val Accuracy=0.9872]\n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.60it/s, Epoch=12, LR=0.000181, Total Loss=1.1895, Total Loss2=1.1589, Mean Accuracy=0.9902]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.04it/s, Epoch=12, Total Val Loss=0.9329, Total Val Loss2=0.9023, Mean Val Accuracy=0.9908]\n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.61it/s, Epoch=13, LR=0.000819, Total Loss=1.15, Total Loss2=1.1199, Mean Accuracy=0.9898]  \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.05it/s, Epoch=13, Total Val Loss=1.0538, Total Val Loss2=1.0196, Mean Val Accuracy=0.9889]\n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.60it/s, Epoch=14, LR=0.000938, Total Loss=1.1269, Total Loss2=1.0994, Mean Accuracy=0.9911]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.05it/s, Epoch=14, Total Val Loss=1.0793, Total Val Loss2=1.0401, Mean Val Accuracy=0.9818]\n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.60it/s, Epoch=15, LR=0.000345, Total Loss=1.0959, Total Loss2=1.0687, Mean Accuracy=0.9913]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.04it/s, Epoch=15, Total Val Loss=0.9397, Total Val Loss2=0.9095, Mean Val Accuracy=0.9914]\n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.60it/s, Epoch=16, LR=4e-06, Total Loss=1.0453, Total Loss2=1.0201, Mean Accuracy=0.9938]   \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.06it/s, Epoch=16, Total Val Loss=0.8076, Total Val Loss2=0.7896, Mean Val Accuracy=0.9962]\n",
      "100%|██████████| 1022/1022 [02:33<00:00,  6.64it/s, Epoch=17, LR=0.000469, Total Loss=1.0294, Total Loss2=1.005, Mean Accuracy=0.9927] \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.05it/s, Epoch=17, Total Val Loss=0.8104, Total Val Loss2=0.7884, Mean Val Accuracy=0.9945]\n",
      "100%|██████████| 1022/1022 [02:33<00:00,  6.65it/s, Epoch=18, LR=0.000984, Total Loss=1.0087, Total Loss2=0.9844, Mean Accuracy=0.993] \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.05it/s, Epoch=18, Total Val Loss=0.793, Total Val Loss2=0.7707, Mean Val Accuracy=0.9932] \n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.59it/s, Epoch=19, LR=0.000713, Total Loss=1.0039, Total Loss2=0.9804, Mean Accuracy=0.9928]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.04it/s, Epoch=19, Total Val Loss=0.9455, Total Val Loss2=0.9238, Mean Val Accuracy=0.9942]\n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.56it/s, Epoch=20, LR=9.5e-05, Total Loss=0.9657, Total Loss2=0.9424, Mean Accuracy=0.9936] \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.01it/s, Epoch=20, Total Val Loss=0.8439, Total Val Loss2=0.823, Mean Val Accuracy=0.9952] \n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.58it/s, Epoch=21, LR=0.000136, Total Loss=0.9337, Total Loss2=0.9127, Mean Accuracy=0.9936]\n",
      "100%|██████████| 365/365 [01:00<00:00,  6.00it/s, Epoch=21, Total Val Loss=0.706, Total Val Loss2=0.6806, Mean Val Accuracy=0.9935] \n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.57it/s, Epoch=22, LR=0.000768, Total Loss=0.9558, Total Loss2=0.9337, Mean Accuracy=0.9935]\n",
      "100%|██████████| 365/365 [01:01<00:00,  5.98it/s, Epoch=22, Total Val Loss=0.8169, Total Val Loss2=0.7977, Mean Val Accuracy=0.9955]\n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.56it/s, Epoch=23, LR=0.000965, Total Loss=0.9406, Total Loss2=0.9171, Mean Accuracy=0.9922]\n",
      "100%|██████████| 365/365 [01:01<00:00,  5.97it/s, Epoch=23, Total Val Loss=1.1582, Total Val Loss2=1.1277, Mean Val Accuracy=0.9896]\n",
      "100%|██████████| 1022/1022 [02:36<00:00,  6.54it/s, Epoch=24, LR=0.000406, Total Loss=0.906, Total Loss2=0.8841, Mean Accuracy=0.9938] \n",
      "100%|██████████| 365/365 [01:01<00:00,  5.97it/s, Epoch=24, Total Val Loss=0.738, Total Val Loss2=0.7167, Mean Val Accuracy=0.9945] \n",
      "100%|██████████| 1022/1022 [02:36<00:00,  6.54it/s, Epoch=25, LR=0.0, Total Loss=0.8961, Total Loss2=0.8746, Mean Accuracy=0.993]     \n",
      "100%|██████████| 365/365 [01:01<00:00,  5.96it/s, Epoch=25, Total Val Loss=0.7617, Total Val Loss2=0.7373, Mean Val Accuracy=0.9932]\n",
      "100%|██████████| 1022/1022 [02:36<00:00,  6.53it/s, Epoch=26, LR=0.000406, Total Loss=0.8955, Total Loss2=0.8755, Mean Accuracy=0.9943]\n",
      "100%|██████████| 365/365 [01:01<00:00,  5.96it/s, Epoch=26, Total Val Loss=0.7942, Total Val Loss2=0.7729, Mean Val Accuracy=0.994] \n",
      "/opt/conda/lib/python3.7/site-packages/albumentations/imgaug/transforms.py:222: FutureWarning: IAASharpen is deprecated. Please use Sharpen instead\n",
      "  warnings.warn(\"IAASharpen is deprecated. Please use Sharpen instead\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/albumentations/imgaug/transforms.py:165: FutureWarning: This augmentation is deprecated. Please use Emboss instead\n",
      "  warnings.warn(\"This augmentation is deprecated. Please use Emboss instead\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:645: FutureWarning: This class has been deprecated. Please use CoarseDropout\n",
      "  FutureWarning,\n",
      "  0%|          | 0/1022 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "100%|██████████| 1022/1022 [02:36<00:00,  6.53it/s, Epoch=1, LR=0.000594, Total Loss=2.9438, Total Loss2=2.8455, Mean Accuracy=0.9591]\n",
      "100%|██████████| 365/365 [01:01<00:00,  5.91it/s, Epoch=1, Total Val Loss=2.537, Total Val Loss2=2.4501, Mean Val Accuracy=0.9603] \n",
      "100%|██████████| 1022/1022 [02:40<00:00,  6.37it/s, Epoch=2, LR=3.5e-05, Total Loss=2.0911, Total Loss2=2.0292, Mean Accuracy=0.9765] \n",
      "100%|██████████| 365/365 [01:01<00:00,  5.89it/s, Epoch=2, Total Val Loss=1.5627, Total Val Loss2=1.5242, Mean Val Accuracy=0.9851]\n",
      "100%|██████████| 1022/1022 [02:38<00:00,  6.43it/s, Epoch=3, LR=0.000232, Total Loss=1.829, Total Loss2=1.7768, Mean Accuracy=0.9795] \n",
      "100%|██████████| 365/365 [01:01<00:00,  5.92it/s, Epoch=3, Total Val Loss=1.434, Total Val Loss2=1.3943, Mean Val Accuracy=0.9877] \n",
      "100%|██████████| 1022/1022 [02:40<00:00,  6.37it/s, Epoch=4, LR=0.000864, Total Loss=1.7189, Total Loss2=1.6702, Mean Accuracy=0.9817]\n",
      "100%|██████████| 365/365 [01:01<00:00,  5.96it/s, Epoch=4, Total Val Loss=1.3354, Total Val Loss2=1.2859, Mean Val Accuracy=0.981] \n",
      "100%|██████████| 1022/1022 [02:36<00:00,  6.51it/s, Epoch=5, LR=0.000905, Total Loss=1.6255, Total Loss2=1.58, Mean Accuracy=0.9829]  \n",
      "100%|██████████| 365/365 [01:01<00:00,  5.95it/s, Epoch=5, Total Val Loss=2.342, Total Val Loss2=2.2803, Mean Val Accuracy=0.9728] \n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.55it/s, Epoch=6, LR=0.000287, Total Loss=1.5326, Total Loss2=1.4915, Mean Accuracy=0.9839]\n",
      "100%|██████████| 365/365 [01:01<00:00,  5.95it/s, Epoch=6, Total Val Loss=1.4091, Total Val Loss2=1.3732, Mean Val Accuracy=0.9901]\n",
      "100%|██████████| 1022/1022 [02:43<00:00,  6.27it/s, Epoch=7, LR=1.6e-05, Total Loss=1.4749, Total Loss2=1.4349, Mean Accuracy=0.9864] \n",
      "100%|██████████| 365/365 [01:03<00:00,  5.77it/s, Epoch=7, Total Val Loss=1.1436, Total Val Loss2=1.1055, Mean Val Accuracy=0.9901]\n",
      "100%|██████████| 1022/1022 [02:47<00:00,  6.09it/s, Epoch=8, LR=0.000531, Total Loss=1.3819, Total Loss2=1.343, Mean Accuracy=0.9866] \n",
      "100%|██████████| 365/365 [01:03<00:00,  5.77it/s, Epoch=8, Total Val Loss=1.2462, Total Val Loss2=1.21, Mean Val Accuracy=0.9875]  \n",
      "100%|██████████| 1022/1022 [02:47<00:00,  6.12it/s, Epoch=9, LR=0.000996, Total Loss=1.3543, Total Loss2=1.316, Mean Accuracy=0.9873] \n",
      "100%|██████████| 365/365 [01:02<00:00,  5.88it/s, Epoch=9, Total Val Loss=1.2047, Total Val Loss2=1.1576, Mean Val Accuracy=0.9812]\n",
      "100%|██████████| 1022/1022 [02:36<00:00,  6.53it/s, Epoch=10, LR=0.000655, Total Loss=1.2937, Total Loss2=1.2601, Mean Accuracy=0.9892]\n",
      "100%|██████████| 365/365 [01:01<00:00,  5.97it/s, Epoch=10, Total Val Loss=1.3074, Total Val Loss2=1.2643, Mean Val Accuracy=0.9851]\n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.57it/s, Epoch=11, LR=6.2e-05, Total Loss=1.2394, Total Loss2=1.2058, Mean Accuracy=0.9885] \n",
      "100%|██████████| 365/365 [01:01<00:00,  5.97it/s, Epoch=11, Total Val Loss=1.1213, Total Val Loss2=1.0886, Mean Val Accuracy=0.9892]\n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.57it/s, Epoch=12, LR=0.000181, Total Loss=1.2089, Total Loss2=1.1763, Mean Accuracy=0.9891]\n",
      "100%|██████████| 365/365 [01:00<00:00,  5.99it/s, Epoch=12, Total Val Loss=1.0278, Total Val Loss2=0.9936, Mean Val Accuracy=0.9902]\n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.60it/s, Epoch=13, LR=0.000819, Total Loss=1.1731, Total Loss2=1.1412, Mean Accuracy=0.9893]\n",
      "100%|██████████| 365/365 [01:01<00:00,  5.97it/s, Epoch=13, Total Val Loss=1.0452, Total Val Loss2=1.0101, Mean Val Accuracy=0.9899]\n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.61it/s, Epoch=14, LR=0.000938, Total Loss=1.1401, Total Loss2=1.1094, Mean Accuracy=0.9895]\n",
      "100%|██████████| 365/365 [01:01<00:00,  5.96it/s, Epoch=14, Total Val Loss=1.2646, Total Val Loss2=1.2309, Mean Val Accuracy=0.9863]\n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.57it/s, Epoch=15, LR=0.000345, Total Loss=1.1379, Total Loss2=1.108, Mean Accuracy=0.9904] \n",
      "100%|██████████| 365/365 [01:01<00:00,  5.94it/s, Epoch=15, Total Val Loss=0.9686, Total Val Loss2=0.9371, Mean Val Accuracy=0.9892]\n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.56it/s, Epoch=16, LR=4e-06, Total Loss=1.0792, Total Loss2=1.0513, Mean Accuracy=0.9911]   \n",
      "100%|██████████| 365/365 [01:01<00:00,  5.94it/s, Epoch=16, Total Val Loss=0.9087, Total Val Loss2=0.8818, Mean Val Accuracy=0.9913]\n",
      "100%|██████████| 1022/1022 [02:36<00:00,  6.55it/s, Epoch=17, LR=0.000469, Total Loss=1.0561, Total Loss2=1.0297, Mean Accuracy=0.9913]\n",
      "100%|██████████| 365/365 [01:01<00:00,  5.94it/s, Epoch=17, Total Val Loss=0.9226, Total Val Loss2=0.9005, Mean Val Accuracy=0.995] \n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.58it/s, Epoch=18, LR=0.000984, Total Loss=1.0425, Total Loss2=1.0168, Mean Accuracy=0.9916]\n",
      "100%|██████████| 365/365 [01:01<00:00,  5.95it/s, Epoch=18, Total Val Loss=0.9388, Total Val Loss2=0.9182, Mean Val Accuracy=0.9947]\n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.60it/s, Epoch=19, LR=0.000713, Total Loss=1.0006, Total Loss2=0.9787, Mean Accuracy=0.9934]\n",
      "100%|██████████| 365/365 [01:01<00:00,  5.94it/s, Epoch=19, Total Val Loss=0.9948, Total Val Loss2=0.9587, Mean Val Accuracy=0.9884]\n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.61it/s, Epoch=20, LR=9.5e-05, Total Loss=1.0079, Total Loss2=0.9827, Mean Accuracy=0.992]  \n",
      "100%|██████████| 365/365 [01:01<00:00,  5.94it/s, Epoch=20, Total Val Loss=0.7778, Total Val Loss2=0.7568, Mean Val Accuracy=0.9955]\n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.56it/s, Epoch=21, LR=0.000136, Total Loss=0.9699, Total Loss2=0.9448, Mean Accuracy=0.9931]\n",
      "100%|██████████| 365/365 [01:01<00:00,  5.91it/s, Epoch=21, Total Val Loss=0.869, Total Val Loss2=0.8444, Mean Val Accuracy=0.9952] \n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.56it/s, Epoch=22, LR=0.000768, Total Loss=0.9665, Total Loss2=0.9437, Mean Accuracy=0.9924]\n",
      "100%|██████████| 365/365 [01:01<00:00,  5.90it/s, Epoch=22, Total Val Loss=0.8391, Total Val Loss2=0.8175, Mean Val Accuracy=0.9947]\n",
      "100%|██████████| 1022/1022 [02:36<00:00,  6.52it/s, Epoch=23, LR=0.000965, Total Loss=0.9552, Total Loss2=0.9304, Mean Accuracy=0.9924]\n",
      "100%|██████████| 365/365 [01:01<00:00,  5.93it/s, Epoch=23, Total Val Loss=0.9336, Total Val Loss2=0.9103, Mean Val Accuracy=0.9938]\n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.60it/s, Epoch=24, LR=0.000406, Total Loss=0.9503, Total Loss2=0.9254, Mean Accuracy=0.9927]\n",
      "100%|██████████| 365/365 [01:01<00:00,  5.93it/s, Epoch=24, Total Val Loss=0.818, Total Val Loss2=0.7991, Mean Val Accuracy=0.9945] \n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.60it/s, Epoch=25, LR=0.0, Total Loss=0.9112, Total Loss2=0.8902, Mean Accuracy=0.9936]     \n",
      "100%|██████████| 365/365 [01:01<00:00,  5.92it/s, Epoch=25, Total Val Loss=0.7506, Total Val Loss2=0.7343, Mean Val Accuracy=0.9962]\n",
      "100%|██████████| 1022/1022 [02:35<00:00,  6.55it/s, Epoch=26, LR=0.000406, Total Loss=0.9094, Total Loss2=0.8882, Mean Accuracy=0.9939]\n",
      "100%|██████████| 365/365 [01:02<00:00,  5.88it/s, Epoch=26, Total Val Loss=0.6726, Total Val Loss2=0.6562, Mean Val Accuracy=0.9971]\n",
      "100%|██████████| 1022/1022 [02:38<00:00,  6.45it/s, Epoch=27, LR=0.000965, Total Loss=0.8945, Total Loss2=0.873, Mean Accuracy=0.9936] \n",
      "100%|██████████| 365/365 [01:01<00:00,  5.90it/s, Epoch=27, Total Val Loss=0.7518, Total Val Loss2=0.7306, Mean Val Accuracy=0.9921]\n",
      "100%|██████████| 1022/1022 [02:37<00:00,  6.48it/s, Epoch=28, LR=0.000768, Total Loss=0.899, Total Loss2=0.8783, Mean Accuracy=0.9938] \n",
      "100%|██████████| 365/365 [01:01<00:00,  5.92it/s, Epoch=28, Total Val Loss=0.8525, Total Val Loss2=0.8379, Mean Val Accuracy=0.9969]\n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.60it/s, Epoch=29, LR=0.000136, Total Loss=0.89, Total Loss2=0.8713, Mean Accuracy=0.994]   \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.01it/s, Epoch=29, Total Val Loss=0.7428, Total Val Loss2=0.7196, Mean Val Accuracy=0.9933]\n",
      "100%|██████████| 1022/1022 [02:34<00:00,  6.61it/s, Epoch=30, LR=9.5e-05, Total Loss=0.881, Total Loss2=0.8605, Mean Accuracy=0.9935] \n",
      "100%|██████████| 365/365 [01:00<00:00,  6.03it/s, Epoch=30, Total Val Loss=0.6798, Total Val Loss2=0.6602, Mean Val Accuracy=0.9942]\n"
     ]
    }
   ],
   "source": [
    "p = 5\n",
    "folds = [0,1,2,3,4]\n",
    "nets = [\"resnet101\", \"resnet50\"]\n",
    "\n",
    "for net in nets:\n",
    "    random_seed = 123\n",
    "    seed_everything(random_seed)\n",
    "    print(\"=================\",net,\"=================\")\n",
    "    tr_path = \"../train_dataset/\"\n",
    "    tr_file = \"train_data.csv\"\n",
    "\n",
    "    pms = []\n",
    "    cls = []\n",
    "    splits = [\"BC\", \"LT\"]\n",
    "\n",
    "    for split in splits:\n",
    "        path = \"../train_dataset/{}\".format(split)\n",
    "\n",
    "        folders = glob.glob(os.path.join(path, \"*\"))\n",
    "\n",
    "        dirs = [os.path.join(f, \"resized512\") for f in folders]\n",
    "\n",
    "        for d in dirs:\n",
    "            permute = permutations(glob.glob(os.path.join(d, \"*.png\")),2)\n",
    "            pm = list(permute)\n",
    "            pms.extend(pm)\n",
    "            cls.extend([int(split == \"BC\")] * len(pm))\n",
    "    \n",
    "    tr_data = pd.DataFrame(pms)\n",
    "    tr_data.columns = [\"before_file_path\", \"after_file_path\"]\n",
    "    tr_data = tr_data[[\"before_file_path\", \"after_file_path\"]]\n",
    "    tr_data['cls'] = cls\n",
    "\n",
    "    tt_before = tr_data['before_file_path'].apply(lambda x: str(x).split(\"DAT\")[1])\n",
    "    tt_before = tt_before.apply(lambda x: int(str(x).split(\".\")[0]))\n",
    "    tt_after = tr_data['after_file_path'].apply(lambda x: str(x).split(\"DAT\")[1])\n",
    "    tt_after = tt_after.apply(lambda x: int(str(x).split(\".\")[0]))\n",
    "\n",
    "    tr_data['label'] = np.int_(tt_before - tt_after < 0)\n",
    "    tr_data['before'] = tt_before\n",
    "    tr_data['after'] = tt_after\n",
    "    tr_data['delta'] = tt_after - tt_before\n",
    "    \n",
    "    tr_data['folder']= tr_data['before_file_path'].apply(lambda x: x.split(\"DAT\")[0])\n",
    "    \n",
    "    tr_data = tr_data.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "\n",
    "    gkf = StratifiedKFold(n_splits=5, random_state=random_seed, shuffle = True)\n",
    "    tr_data['fold'] = -1\n",
    "    for fold, (train_idx, val_idx) in enumerate(gkf.split(tr_data, tr_data.label)):\n",
    "        tr_data.loc[val_idx, 'fold'] = fold\n",
    "\n",
    "    for fold in folds:\n",
    "        valid_df = tr_data[tr_data['fold'] == fold]\n",
    "        train_df = tr_data[tr_data['fold'] != fold]\n",
    "\n",
    "        image_size = 448\n",
    "        batch_size = 16\n",
    "\n",
    "        train_df = train_df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "        valid_df = valid_df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "\n",
    "        train_dataset = CropDataset(\n",
    "            df=train_df, image_size=image_size, mode='train', f=0.7)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                  sampler=RandomSampler(train_dataset), num_workers = 4, drop_last=True)\n",
    "        valid_dataset = CropDataset(\n",
    "            df=valid_df, image_size=image_size, mode='valid', f=1)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=batch_size, \n",
    "                                  sampler=RandomSampler(valid_dataset), num_workers = 4, drop_last=True)\n",
    "\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        model = CNN2RNN_Network(net,512,image_size)\n",
    "        model = model.to(device)\n",
    "        model_ft = model.feature_extract_model\n",
    "        ct = 0\n",
    "        for child in model_ft.children():\n",
    "            ct += 1\n",
    "            if ct < 8:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "\n",
    "        epochs = 30\n",
    "        aux_epochs = 50\n",
    "        learning_rate = 0.001\n",
    "        save_path = './models/resnet_bestmodel_fold{}_{}.pt'.format(fold, net)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, aux_epochs)\n",
    "        criterion1 = nn.BCEWithLogitsLoss()\n",
    "        criterion2 = nn.L1Loss()\n",
    "\n",
    "        loss_plot, val_loss_plot = [], []\n",
    "        patience = 0\n",
    "        for epoch in range(epochs):\n",
    "            total_loss, total_loss2, total_val_loss, total_val_loss2 = 0, 0, 0, 0\n",
    "            total_correct, total_val_correct = 0, 0\n",
    "\n",
    "            len_train = len(train_loader)\n",
    "            tqdm_dataset = tqdm(enumerate(train_loader), total=len_train)\n",
    "            training = True\n",
    "            for batch, batch_item in tqdm_dataset:\n",
    "                batch_loss, batch_loss2, correct = train_step(batch_item, epoch, batch, training, scheduler)\n",
    "                total_loss += batch_loss\n",
    "                total_loss2 += batch_loss2\n",
    "                total_correct += correct\n",
    "\n",
    "                tqdm_dataset.set_postfix({\n",
    "                    'Epoch': epoch + 1,\n",
    "                    'LR' : '{}'.format(round(optimizer.param_groups[0]['lr'],6)),\n",
    "                    'Total Loss' : '{}'.format(round(total_loss/(batch+1),4)),\n",
    "                    'Total Loss2' : '{}'.format(round(total_loss2/(batch+1),4)),\n",
    "                    'Mean Accuracy' : '{}'.format(round(total_correct/((batch+1)*batch_size),4))\n",
    "                })\n",
    "\n",
    "            loss_plot.append(total_loss/(batch+1))\n",
    "\n",
    "            len_val = len(valid_loader)\n",
    "            tqdm_dataset = tqdm(enumerate(valid_loader), total=len_val)\n",
    "            training = False\n",
    "            for batch, batch_item in tqdm_dataset:\n",
    "                batch_loss, batch_loss2, correct = train_step(batch_item, epoch, batch, training, scheduler)\n",
    "                total_val_loss += batch_loss\n",
    "                total_val_correct += correct\n",
    "                total_val_loss2 += batch_loss2\n",
    "                tqdm_dataset.set_postfix({\n",
    "                    'Epoch': epoch + 1,\n",
    "                    'Total Val Loss' : '{}'.format(round(total_val_loss/(batch+1),4)),\n",
    "                    'Total Val Loss2' : '{}'.format(round(total_val_loss2/(batch+1),4)),\n",
    "                    'Mean Val Accuracy' : '{}'.format(round(total_val_correct/((batch+1)*batch_size),4))\n",
    "                })\n",
    "            val_loss_plot.append(total_val_loss/(batch+1))\n",
    "\n",
    "            if np.min(val_loss_plot) == val_loss_plot[-1]:\n",
    "                patience = 0\n",
    "                torch.save(model, save_path)\n",
    "            if np.min(val_loss_plot) < val_loss_plot[-1]:\n",
    "                patience += 1\n",
    "\n",
    "            if patience >= p:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "voluntary-administrator",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropTestDataset(Dataset):\n",
    "    def __init__(self, df, image_size, mode):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        assert mode in [\"train\", \"valid\", \"test\"] \n",
    "        self.mode = mode\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        self.transform = albu.Compose([\n",
    "            albu.Resize(self.image_size, self.image_size),\n",
    "            albu.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "            ToTensorV2(),\n",
    "        ], additional_targets={'image1': 'image'})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        before_img_path = self.df.loc[index, 'before_file_path']\n",
    "        after_img_path = self.df.loc[index, 'after_file_path']\n",
    "\n",
    "        before_img = cv2.imread(before_img_path)\n",
    "        after_img = cv2.imread(after_img_path)\n",
    "        \n",
    "        aux_input1 = torch.FloatTensor([self.df.loc[index, \"cls\"]])\n",
    "\n",
    "        file_size1 = os.path.getsize(before_img_path)\n",
    "        file_size2 = os.path.getsize(after_img_path)\n",
    "        aux_input2 = torch.FloatTensor([file_size2/10000 - file_size1/10000])\n",
    "        \n",
    "        transformed = self.transform(image=before_img, image1=after_img)\n",
    "        \n",
    "        before_img = transformed['image'] \n",
    "        after_img = transformed['image1']\n",
    "\n",
    "        indice = torch.FloatTensor([self.df.loc[index, 'idx']])\n",
    "        \n",
    "        return before_img, after_img, aux_input1, aux_input2, indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sublime-blame",
   "metadata": {},
   "outputs": [],
   "source": [
    "te_path = \"../test_dataset/\"\n",
    "te_file = \"test_data.csv\"\n",
    "te_data = pd.read_csv(os.path.join(te_path,te_file))\n",
    "te_data[\"before_file_path\"] = te_data[\"before_file_path\"].apply(lambda x: str(x) + \".png\")\n",
    "te_data[\"before_file_path\"] = te_data[\"before_file_path\"].apply(lambda x: os.path.join(\"../test_dataset\",\n",
    "                                                               str(x).split(\"_\")[1],\n",
    "                                                               str(x).split(\"_\")[2],\n",
    "                                                               \"resized512\",\n",
    "                                                               str(x)))\n",
    "\n",
    "te_data[\"after_file_path\"] = te_data[\"after_file_path\"].apply(lambda x: str(x)+\".png\")\n",
    "te_data[\"after_file_path\"] = te_data[\"after_file_path\"].apply(lambda x: os.path.join(\"../test_dataset\",\n",
    "                                                               str(x).split(\"_\")[1],\n",
    "                                                               str(x).split(\"_\")[2],\n",
    "                                                                \"resized512\",\n",
    "                                                               str(x)))\n",
    "te_data['cls']= np.int_(te_data['before_file_path'].apply(lambda x: x.split(\"/\")[2]) == \"BC\")\n",
    "\n",
    "image_size = 448\n",
    "batch_size = 2\n",
    "\n",
    "test_dataset = CropTestDataset(df=te_data, image_size=image_size, mode='test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, drop_last=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dental-means",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1980/1980 [01:31<00:00, 21.55it/s]\n",
      "100%|██████████| 1980/1980 [01:29<00:00, 22.05it/s]\n",
      "100%|██████████| 1980/1980 [01:30<00:00, 21.95it/s]\n",
      "100%|██████████| 1980/1980 [01:30<00:00, 21.85it/s]\n",
      "100%|██████████| 1980/1980 [01:30<00:00, 21.91it/s]\n",
      "100%|██████████| 1980/1980 [00:54<00:00, 36.25it/s]\n",
      "100%|██████████| 1980/1980 [00:53<00:00, 37.03it/s]\n",
      "100%|██████████| 1980/1980 [00:53<00:00, 37.33it/s]\n",
      "100%|██████████| 1980/1980 [00:53<00:00, 37.23it/s]\n",
      "100%|██████████| 1980/1980 [00:53<00:00, 37.33it/s]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./models/\"\n",
    "device = torch.device(\"cuda:0\")\n",
    "folds = [0,1,2,3,4]\n",
    "\n",
    "mean_pred = []\n",
    "\n",
    "for net in nets:\n",
    "    outs = []\n",
    "    idxs = []\n",
    "    for fold in folds:\n",
    "        model = torch.load(model_path + 'resnet_bestmodel_fold{}_{}.pt'.format(str(fold),net))\n",
    "        inf = []\n",
    "        idx = []\n",
    "        len_test = len(test_loader)\n",
    "        tqdm_dataset = tqdm(test_loader, total=len_test)\n",
    "        for before_img, after_img, aux_input1, aux_input2, indice in tqdm_dataset:\n",
    "            before_img = before_img.to(device)\n",
    "            after_img = after_img.to(device)\n",
    "            aux_input1 = aux_input1.to(device)\n",
    "            aux_input2 = aux_input2.to(device)\n",
    "            img = torch.stack([before_img, after_img])\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                output = model(img, aux_input1, aux_input2)\n",
    "                _, delta = output\n",
    "                delta = delta.detach().cpu().numpy()\n",
    "            inf.append(delta)\n",
    "            idx.append(indice)\n",
    "        outs.append(np.concatenate(inf))\n",
    "        idxs.append(np.concatenate(idx))\n",
    "    out_pred = np.array(outs).mean(axis=0)\n",
    "    idx = np.array(idxs).mean(axis=0)\n",
    "    mean_pred.append(out_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cordless-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"../sample_submission.csv\")\n",
    "sub['time_delta'] = np.mean(mean_pred, axis= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "expressed-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(\"sample_submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
