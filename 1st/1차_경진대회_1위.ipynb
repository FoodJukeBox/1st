{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b38d2a8",
   "metadata": {},
   "source": [
    "*1st place solution*  \n",
    "*Team: vecxoz*  \n",
    "*License: MIT*  \n",
    "\n",
    "### Summary\n",
    "\n",
    "Many thanks to the hosts for the very interesting challenge and congrats to all participants!\n",
    "\n",
    "My solution is based on CNN-LSTM architecture with EfficientNet-B7 backbone trained on pairs of 512x512 images. I used original images without cleaning and any preprocessing. Dataset consists of 14613 pairs total: 11k for training and 3k for validation. I built a 5-fold cross-validation setup and tried to ensemble 5 models from each fold, but the ensemble was not better than a single 1st fold. As augmentation I used flips and rotations multiple of 90 degrees. Optimization performed with Adam optimizer and constant learning rate. Architecture has a substantial capacity due to a large backbone and large LSTM block and learns very quickly (1 epoch). \n",
    "\n",
    "A significant improvement in the score was obtained using linear calibration. From the visual inspection of the test set itâ€™s clear that plants `LT_1088` and `LT_1089` have different appearance and evolution patterns compared to `LT` plants present in the training set. Here we have classic examples of so-called \"data drift\" i.e. distribution shift of the test set. There are different approaches which allow accommodating new distribution but I used simple calibration with linear coefficients applied to the predictions of my model. Depending on data split and seeds, the raw private score of my model is around 5.1 and calibrated private score is around 4.2. \n",
    "\n",
    "Acknowledgement. Thanks to [TRC program]( https://sites.research.google/trc/about/) I had an opportunity to run experiments on TPUv3-8.\n",
    "\n",
    "### System requirements\n",
    "```\n",
    "12 CPU, 48 GB RAM, V100-32GB GPU  \n",
    "\n",
    "Ubuntu 18.04  \n",
    "Python: 3.6.9  \n",
    "CUDA: 11.1  \n",
    "cuDNN: 8.0.4  \n",
    "\n",
    "numpy==1.19.5  \n",
    "pandas==1.1.5  \n",
    "Pillow==8.2.0  \n",
    "scikit-learn==0.24.2  \n",
    "tensorflow==2.4.1  \n",
    "tensorflow-addons==0.12.0  \n",
    "efficientnet==1.1.1  \n",
    "\n",
    "Execution time: <1 hour  \n",
    "```\n",
    "\n",
    "### Environment setup\n",
    "\n",
    "```\n",
    "mkdir $HOME/solution\n",
    "cd $HOME/solution\n",
    "# Donload file \"open.zip\"\n",
    "unzip open.zip\n",
    "mv open/train_dataset open/test_dataset open/sample_submission.csv ./\n",
    "rm -rf open.zip open\n",
    "# Copy this notebook into $HOME/solution\n",
    "# Now we have the following structure:\n",
    "\n",
    "$HOME/\n",
    "    solution/\n",
    "        test_dataset/\n",
    "        train_dataset/\n",
    "        notebook.ipynb # this notebook\n",
    "        sample_submission.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c6cfac",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ae51b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import tensorflow as tf\n",
    "print('tf:', tf.__version__)\n",
    "import tensorflow_addons as tfa\n",
    "print('tfa:', tfa.__version__)\n",
    "import efficientnet.tfkeras as efn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e3d59c",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ada3ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    data_dir = os.path.join(os.path.expanduser('~'), 'solution')             # Directory containig CSV files\n",
    "    data_tfrec_dir = os.path.join(os.path.expanduser('~'), 'solution/tfrec') # Directory containig TFRecord files\n",
    "    data_preds_dir = os.path.join(os.path.expanduser('~'), 'solution/preds') # Directory where predictions will be saved\n",
    "    seed = 547895                     # Seed\n",
    "    mixed_precision = 'mixed_float16' # Mixed precision. E.g.: mixed_float16, mixed_bfloat16, or None\n",
    "    job = 'train_test'                # Job to run. Possible values: train, test, or train_test\n",
    "    n_folds = 5                       # Number of folds\n",
    "    initial_fold = 0                  # Initial fold (from 0)\n",
    "    final_fold = 1                    # Final fold (from 1). To train all folds set equal to n_folds\n",
    "    n_channels = 3                    # Number of image channels\n",
    "    volume = 2                        # Number of 3-channel images in a sequence (volume)\n",
    "    dim = 512                         # Image size in pixels\n",
    "    n_examples_train = 11_691         # Number of training examples. This value is used to define an epoch\n",
    "    n_epochs = 1                      # Number of epochs to train\n",
    "    batch_size = 8                    # Batch size\n",
    "    lr = 1e-4                         # Learning rate\n",
    "    aug_number = 5                    # Number of train-time augmentations. 0 means no aug\n",
    "    tta_number = 0                    # Number of test-time augmentations. 0 means no tta\n",
    "    buffer_size = 1024                # Shuffle buffer size for tf.data\n",
    "    create_tfrecords = True           # True to create TFRecords (first run), False to skip (next runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be449e5",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2026896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nCr(n, r):\n",
    "    \"\"\"\n",
    "    Number of combinations from n by r\n",
    "    \"\"\"\n",
    "    return math.factorial(n) // math.factorial(r) // math.factorial(n-r)\n",
    "\n",
    "\n",
    "def seeder(seed):\n",
    "    \"\"\"\n",
    "    Set seed\n",
    "    \"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    return seed\n",
    "\n",
    "\n",
    "class TFRecordProcessor(object):\n",
    "    \"\"\"\n",
    "    Write TFRecord files based on Pandas dataframe\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.n_examples = 0\n",
    "    def _bytes_feature(self, value):\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "    def _int_feature(self, value):\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "    def _float_feature(self, value):\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "    def _process_example(self, ind, A, B, C, D):\n",
    "        self.n_examples += 1\n",
    "        feature = collections.OrderedDict()\n",
    "        feature['image_id'] = self._bytes_feature([A[ind].encode('utf-8')])\n",
    "        feature['image'] =    self._bytes_feature([tf.io.read_file(B[ind][0]).numpy(), \n",
    "                                                   tf.io.read_file(B[ind][1]).numpy()])\n",
    "        feature['label_id'] = self._bytes_feature([C[ind].encode('utf-8')])\n",
    "        feature['label'] =    self._int_feature(D[ind])\n",
    "        example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        self._writer.write(example_proto.SerializeToString())\n",
    "    def write_tfrecords(self, A, B, C, D, n_shards=1, file_out='train.tfrecord'):\n",
    "        n_examples_per_shard = A.shape[0] // n_shards\n",
    "        n_examples_remainder = A.shape[0] %  n_shards   \n",
    "        self.n_examples = 0\n",
    "        for shard in range(n_shards):\n",
    "            self._writer = tf.io.TFRecordWriter('%s-%05d-of-%05d' % (file_out, shard, n_shards))\n",
    "            start = shard * n_examples_per_shard\n",
    "            if shard == (n_shards - 1):\n",
    "                end = (shard + 1) * n_examples_per_shard + n_examples_remainder\n",
    "            else:\n",
    "                end = (shard + 1) * n_examples_per_shard\n",
    "            print('Shard %d of %d: (%d examples)' % (shard, n_shards, (end - start)))\n",
    "            for i in range(start, end):\n",
    "                self._process_example(i, A, B, C, D)\n",
    "                print(i, end='\\r')\n",
    "            self._writer.close()\n",
    "        return self.n_examples\n",
    "\n",
    "\n",
    "def init_tfdata(files_glob, deterministic=True, batch_size=32, auto=-1, \n",
    "                parse_example=None, mod=None, aug=None, tta=None, norm=None, \n",
    "                repeat=False, buffer_size=None, cache=False):\n",
    "    \"\"\"\n",
    "    Init tf.data.TFRecordDataset\n",
    "    \"\"\"\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_deterministic = deterministic\n",
    "    files = tf.data.Dataset.list_files(files_glob, \n",
    "                                       shuffle=not deterministic).with_options(options)\n",
    "    print('N tfrec files:', len(files))\n",
    "    #\n",
    "    ds = tf.data.TFRecordDataset(files, num_parallel_reads=auto)\n",
    "    ds = ds.with_options(options)\n",
    "    ds = ds.map(parse_example, num_parallel_calls=auto)\n",
    "    #\n",
    "    if mod:\n",
    "        ds = ds.map(mod, num_parallel_calls=auto)\n",
    "    if aug:\n",
    "        ds = ds.map(aug, num_parallel_calls=auto)\n",
    "    if tta:\n",
    "        ds = ds.map(tta, num_parallel_calls=auto)\n",
    "    if norm:\n",
    "        ds = ds.map(norm, num_parallel_calls=auto)\n",
    "    if repeat:\n",
    "        ds = ds.repeat()\n",
    "    if buffer_size:\n",
    "        ds = ds.shuffle(buffer_size=buffer_size, \n",
    "                        reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(batch_size=batch_size)\n",
    "    ds = ds.prefetch(auto)\n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "    #\n",
    "    return ds\n",
    "\n",
    "\n",
    "def fn(image):\n",
    "    \"\"\"\n",
    "    Parse JPG image\n",
    "    \"\"\"\n",
    "    image = tf.image.decode_jpeg(image, channels=args.n_channels, dct_method='INTEGER_ACCURATE')\n",
    "    image = tf.image.resize(image, [args.dim, args.dim])\n",
    "    image = tf.cast(image, dtype=tf.uint8)\n",
    "    return image\n",
    "\n",
    "\n",
    "def parse_example(example_proto):\n",
    "    feature_description = {\n",
    "        'image':    tf.io.FixedLenFeature([args.volume], tf.string),\n",
    "        'label':    tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    d = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    image = tf.map_fn(fn, d['image'], dtype=tf.uint8)\n",
    "    image = tf.image.resize(image, [args.dim, args.dim])\n",
    "    image = tf.reshape(image, [args.volume, args.dim, args.dim, args.n_channels])\n",
    "    image = tf.cast(image, tf.uint8)\n",
    "    label = tf.cast(d['label'], tf.int32)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def aug_0(image, label):\n",
    "    return image, label\n",
    "def aug_1(image, label):\n",
    "    return tf.image.flip_left_right(image), label\n",
    "def aug_2(image, label):\n",
    "    return tf.image.flip_up_down(image), label\n",
    "def aug_3(image, label):\n",
    "    return tfa.image.rotate(image, math.pi/4*2), label #  90\n",
    "def aug_4(image, label):\n",
    "    return tfa.image.rotate(image, math.pi/4*4), label # 180\n",
    "def aug_5(image, label):\n",
    "    return tfa.image.rotate(image, math.pi/4*6), label # 270\n",
    "def aug_6(image, label):\n",
    "    return tf.image.central_crop(tfa.image.rotate(image, math.pi/4*1), 0.7), label #  45\n",
    "def aug_7(image, label):\n",
    "    return tf.image.central_crop(tfa.image.rotate(image, math.pi/4*3), 0.7), label # 135\n",
    "def aug_8(image, label):\n",
    "    return tf.image.central_crop(tfa.image.rotate(image, math.pi/4*5), 0.7), label # 225\n",
    "def aug_9(image, label):\n",
    "    return tf.image.central_crop(tfa.image.rotate(image, math.pi/4*7), 0.7), label # 315\n",
    "\n",
    "\n",
    "tta_func_list = [aug_0, aug_1, aug_2, aug_3, aug_4, aug_5, aug_6, aug_7, aug_8, aug_9][:args.tta_number + 1]\n",
    "\n",
    "\n",
    "def aug(image, label):\n",
    "    \"\"\"\n",
    "    Runs single transformation per call with `aug_percentage` probability\n",
    "    Specific transformation probability is `aug_percentage` / `aug_number`\n",
    "    \"\"\"\n",
    "    aug_percentage = 0.5\n",
    "    aug_maxval = round(args.aug_number / aug_percentage)\n",
    "    #\n",
    "    if args.aug_number != 0:\n",
    "        aug_id = tf.random.uniform([], minval=0, maxval=aug_maxval, dtype=tf.int32)\n",
    "    #\n",
    "    if args.aug_number == 0:\n",
    "        pass\n",
    "    elif aug_id == 0:\n",
    "        image, label = aug_1(image, label)\n",
    "    elif aug_id == 1:\n",
    "        image, label = aug_2(image, label)\n",
    "    elif aug_id == 2:\n",
    "        image, label = aug_3(image, label)\n",
    "    elif aug_id == 3:\n",
    "        image, label = aug_4(image, label)\n",
    "    elif aug_id == 4:\n",
    "        image, label = aug_5(image, label)\n",
    "    elif aug_id == 5:\n",
    "        image, label = aug_6(image, label)\n",
    "    elif aug_id == 6:\n",
    "        image, label = aug_7(image, label)\n",
    "    elif aug_id == 7:\n",
    "        image, label = aug_8(image, label)\n",
    "    elif aug_id == 8:\n",
    "        image, label = aug_9(image, label)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def norm(image, label):\n",
    "    image = tf.image.resize(image, [args.dim, args.dim])\n",
    "    image = tf.reshape(image, [args.volume, args.dim, args.dim, args.n_channels])\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    label = tf.cast(label, tf.int32)\n",
    "    image = image / 255.0\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def init_model(print_summary=True):\n",
    "    #\n",
    "    inp = tf.keras.layers.Input(shape=(args.volume, \n",
    "                                       args.dim, \n",
    "                                       args.dim, \n",
    "                                       args.n_channels), dtype=tf.float32)\n",
    "    back = efn.EfficientNetB7(input_shape=(args.dim, \n",
    "                                           args.dim, \n",
    "                                           args.n_channels),\n",
    "                              weights='imagenet', include_top=False)\n",
    "    pool_2d = tf.keras.layers.GlobalAveragePooling2D()\n",
    "    #\n",
    "    x = tf.keras.layers.TimeDistributed(back)(inp)\n",
    "    x = tf.keras.layers.TimeDistributed(pool_2d)(x)\n",
    "    x = tf.keras.layers.LSTM(1024, return_sequences=False)(x)\n",
    "    x = tf.keras.layers.Dense(300, activation='relu')(x)\n",
    "    out = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "    #\n",
    "    model = tf.keras.models.Model(inp, out, name='model')\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(args.lr), \n",
    "                  loss='mse',\n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n",
    "    if print_summary:\n",
    "        model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660c2701",
   "metadata": {},
   "source": [
    "### Create data\n",
    "\n",
    "Here we can skip image resizing and TFRecord creation if already done:\n",
    "```\n",
    "# args.create_tfrecords = False # skip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0426a00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.create_tfrecords:\n",
    "    print('Resise all images to 1024x1024 and compress as JPG to speedup data loading')\n",
    "    for in_dir, out_dir in (('train_dataset', 'train_dataset_jpg_1024'), ('test_dataset', 'test_dataset_jpg_1024')):\n",
    "        print(in_dir, '->', out_dir)\n",
    "        files = sorted(glob.glob(os.path.join(args.data_dir, '%s/*/*/*.png' % in_dir)))\n",
    "        os.makedirs(os.path.join(args.data_dir, out_dir), exist_ok=True)\n",
    "        for counter, file in enumerate(files):\n",
    "            out_file = file.replace(in_dir, out_dir)\n",
    "            out_file = out_file.replace('.png', '.jpg')\n",
    "            os.makedirs(os.path.dirname(out_file), exist_ok=True)\n",
    "            Image.open(file).resize((1024, 1024)).save(out_file)\n",
    "            print(counter, end='\\r')\n",
    "\n",
    "print('Create train pairs (compbinations)')\n",
    "\n",
    "n_files = 0\n",
    "n_combs = 0\n",
    "combs = []\n",
    "for dir_species in sorted(glob.glob(os.path.join(args.data_dir, 'train_dataset_jpg_1024/*'))):\n",
    "    for dir_timeseries in sorted(glob.glob(os.path.join(dir_species, '*'))):\n",
    "        files = sorted(glob.glob(os.path.join(dir_timeseries, '*.jpg')))\n",
    "        combs.extend(list(itertools.combinations(files, 2)))\n",
    "        n = len(files)\n",
    "        n_files += n\n",
    "        n_combs += nCr(n, 2)\n",
    "print('N pairs:', n_combs)\n",
    "\n",
    "print('Create train dataframe')\n",
    "\n",
    "image_id = []\n",
    "image = []\n",
    "label = []\n",
    "label_id = []\n",
    "group = []\n",
    "for counter, comb in enumerate(combs):\n",
    "    image_id.append('comb_%06d' % counter)\n",
    "    image.append(sorted(comb))\n",
    "    delta = int(comb[1].split('.')[-2][-2:]) - int(comb[0].split('.')[-2][-2:])\n",
    "    label.append(delta)\n",
    "    label_id.append(str(delta))\n",
    "    group.append(comb[0].split('/')[-2])\n",
    "\n",
    "train_df = pd.DataFrame()\n",
    "train_df['image_id'] = image_id\n",
    "train_df['image'] = image\n",
    "train_df['label_id'] = label_id\n",
    "train_df['label'] = label\n",
    "train_df['group'] = group\n",
    "train_df['fold_id'] = 0\n",
    "\n",
    "print('Create test dataframe')\n",
    "\n",
    "files = sorted(glob.glob(os.path.join(args.data_dir, 'test_dataset_jpg_1024/*/*/*.jpg')))\n",
    "test_df = pd.read_csv(os.path.join(args.data_dir, 'test_dataset/test_data.csv'))\n",
    "\n",
    "image_id = []\n",
    "image = []\n",
    "label = []\n",
    "label_id = []\n",
    "group = []\n",
    "for counter, row in test_df.iterrows():\n",
    "    image_id.append('comb_%06d' % row['idx'])\n",
    "    pair = []\n",
    "    for file in files:\n",
    "        if row['before_file_path'] in file:\n",
    "            pair.append(file)\n",
    "    for file in files:\n",
    "        if row['after_file_path'] in file:\n",
    "            pair.append(file)\n",
    "    image.append(pair)\n",
    "    label.append(0)\n",
    "    label_id.append('0')\n",
    "    group.append('Unk')\n",
    "\n",
    "test_df = pd.DataFrame()\n",
    "test_df['image_id'] = image_id\n",
    "test_df['image'] = image\n",
    "test_df['label_id'] = label_id\n",
    "test_df['label'] = label\n",
    "test_df['group'] = group\n",
    "test_df['fold_id'] = 0\n",
    "\n",
    "print('Create train split')\n",
    "\n",
    "n_splits = 5\n",
    "kf = GroupKFold(n_splits=n_splits)\n",
    "for fold_id, (train_index, val_index) in enumerate(kf.split(train_df, train_df['label'].values, \n",
    "                                                            groups=train_df['group'].values)):\n",
    "    train_df.loc[train_df.index.isin(val_index), 'fold_id'] = fold_id\n",
    "train_df = train_df.sample(frac=1.0, random_state=34)\n",
    "\n",
    "if args.create_tfrecords:\n",
    "    print('Create TFRecords')\n",
    "    os.makedirs(args.data_tfrec_dir, exist_ok=True)\n",
    "    tfrp = TFRecordProcessor()\n",
    "    \n",
    "    for fold_id in range(len(train_df['fold_id'].unique())):\n",
    "        print('Fold:', fold_id)\n",
    "        n_written = tfrp.write_tfrecords(\n",
    "            train_df[train_df['fold_id'] == fold_id]['image_id'].values,\n",
    "            train_df[train_df['fold_id'] == fold_id]['image'].values,\n",
    "            train_df[train_df['fold_id'] == fold_id]['label_id'].values,\n",
    "            train_df[train_df['fold_id'] == fold_id]['label'].values,\n",
    "            #\n",
    "            n_shards=1, \n",
    "            file_out=os.path.join(args.data_tfrec_dir, 'fold.%d.tfrecord' % fold_id))\n",
    "    \n",
    "    n_written = tfrp.write_tfrecords(\n",
    "        test_df['image_id'].values,\n",
    "        test_df['image'].values,\n",
    "        test_df['label_id'].values,\n",
    "        test_df['label'].values,\n",
    "        #\n",
    "        n_shards=1,\n",
    "        file_out=os.path.join(args.data_tfrec_dir, 'test.tfrecord'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ce67a5",
   "metadata": {},
   "source": [
    "### Train and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b233e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_id in range(args.initial_fold, args.final_fold):\n",
    "    print('\\n*****')\n",
    "    print('Fold:', fold_id)\n",
    "    print('*****\\n')\n",
    "    #--------------------------------------------------------------------------\n",
    "    os.makedirs(args.data_preds_dir, exist_ok=True)\n",
    "    #--------------------------------------------------------------------------\n",
    "    print('Clear session...')\n",
    "    tf.keras.backend.clear_session()\n",
    "    #--------------------------------------------------------------------------\n",
    "    print('Set fold-specific seed...')\n",
    "    _ = seeder(args.seed + fold_id)\n",
    "    #--------------------------------------------------------------------------\n",
    "    print('Allow growth')\n",
    "    tf.config.experimental.set_memory_growth(device=tf.config.list_physical_devices('GPU')[0], enable=True)\n",
    "    # tf.config.set_logical_device_configuration(tf.config.list_physical_devices('GPU')[0], \n",
    "    #     [tf.config.LogicalDeviceConfiguration(memory_limit=30_000)])\n",
    "    #--------------------------------------------------------------------------\n",
    "    if args.mixed_precision is not None:\n",
    "        print('Init mixed precision:', args.mixed_precision)\n",
    "        policy = tf.keras.mixed_precision.experimental.Policy(args.mixed_precision)\n",
    "        tf.keras.mixed_precision.experimental.set_policy(policy)\n",
    "    else:\n",
    "        print('Using default precision:', tf.keras.backend.floatx())\n",
    "    #--------------------------------------------------------------------------\n",
    "    print('FULL BATCH SHAPE: %d x %d x %d x %d x %d' % (args.batch_size,\n",
    "                                                        args.volume,\n",
    "                                                        args.dim,\n",
    "                                                        args.dim,\n",
    "                                                        args.n_channels))\n",
    "    #--------------------------------------------------------------------------\n",
    "    # Init TPU\n",
    "    print('Init accelerator')\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    #--------------------------------------------------------------------------\n",
    "    # Globs\n",
    "    all_fold_ids = np.array(range(args.n_folds))\n",
    "    train_fold_ids = all_fold_ids[all_fold_ids != fold_id]\n",
    "    train_glob = os.path.join(args.data_tfrec_dir, 'fold.[%d%d%d%d].tfrecord*' % tuple(train_fold_ids))\n",
    "    val_glob   = os.path.join(args.data_tfrec_dir, 'fold.[%d].tfrecord*' % fold_id)\n",
    "    test_glob  = os.path.join(args.data_tfrec_dir, 'test.tfrecord*')\n",
    "    print('TRAIN GLOB:', train_glob)\n",
    "    print('VAL   GLOB:', val_glob)\n",
    "    print('TEST  GLOB:', test_glob)\n",
    "    #--------------------------------------------------------------------------\n",
    "    print('Init datasets')\n",
    "    train_ds = init_tfdata(train_glob, \n",
    "                           deterministic=True,  \n",
    "                           batch_size=args.batch_size, \n",
    "                           auto=-1,\n",
    "                           parse_example=parse_example, \n",
    "                           aug=aug, \n",
    "                           norm=norm,\n",
    "                           repeat=True,\n",
    "                           buffer_size=args.buffer_size, \n",
    "                           cache=False)\n",
    "    val_ds = init_tfdata(val_glob, \n",
    "                         deterministic=True,  \n",
    "                         batch_size=args.batch_size, \n",
    "                         auto=-1,\n",
    "                         parse_example=parse_example,\n",
    "                         norm=norm,\n",
    "                         repeat=False,  \n",
    "                         buffer_size=None,\n",
    "                         cache=False)\n",
    "    #--------------------------------------------------------------------------\n",
    "    print('Init model')\n",
    "    with strategy.scope():\n",
    "        model = init_model(print_summary=True)\n",
    "    #--------------------------------------------------------------------------\n",
    "    print('Init callbacks')\n",
    "    weight_file = 'model-f%d-e{epoch:03d}-{val_loss:.4f}-{val_%s:.4f}.h5' % (fold_id, 'rmse')\n",
    "    call_ckpt = tf.keras.callbacks.ModelCheckpoint(weight_file,\n",
    "                                                   monitor='val_rmse',\n",
    "                                                   save_best_only=False,\n",
    "                                                   save_weights_only=True,\n",
    "                                                   mode='auto',\n",
    "                                                   verbose=1)\n",
    "    #-------------------------------------------------------------------------- \n",
    "    if 'train' in args.job:\n",
    "        print('Fit (fold %d)' % fold_id)\n",
    "        h = model.fit(\n",
    "            train_ds,\n",
    "            steps_per_epoch=args.n_examples_train // args.batch_size,\n",
    "            epochs=args.n_epochs,\n",
    "            validation_data=val_ds,\n",
    "            callbacks=[call_ckpt])\n",
    "    #--------------------------------------------------------------------------\n",
    "    # Load best model for fold\n",
    "    m = sorted(glob.glob('model-f%d*.h5' % fold_id))[-1]\n",
    "    print('Load model (fold %d): %s' % (fold_id, m))\n",
    "    model.load_weights(m)\n",
    "    #--------------------------------------------------------------------------\n",
    "    # TTA\n",
    "    #--------------------------------------------------------------------------\n",
    "    for tta_id in range(len(tta_func_list)):\n",
    "        print('Init datasets for prediction (fold %d, tta %d)' % (fold_id, tta_id))\n",
    "        test_ds = init_tfdata(test_glob, \n",
    "                              deterministic=True,  \n",
    "                              batch_size=args.batch_size, \n",
    "                              auto=-1,\n",
    "                              parse_example=parse_example,\n",
    "                              tta=tta_func_list[tta_id],\n",
    "                              norm=norm,\n",
    "                              repeat=False,  \n",
    "                              buffer_size=None,\n",
    "                              cache=False)\n",
    "        #--------------------------------------------------------------------------\n",
    "        # Predict test\n",
    "        if 'test' in args.job:\n",
    "            print('Predict TEST (fold %d, tta %d)' % (fold_id, tta_id))\n",
    "            y_pred_test = model.predict(test_ds, verbose=1)\n",
    "            np.save(os.path.join(args.data_preds_dir, \n",
    "                                 'y_pred_test_fold_%d_tta_%d.npy' % (fold_id, tta_id)), y_pred_test)\n",
    "        #--------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cb401f",
   "metadata": {},
   "source": [
    "### Create raw submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef6a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions and sample submission\n",
    "y_pred = np.load(os.path.join(args.data_preds_dir, 'y_pred_test_fold_0_tta_0.npy'))\n",
    "subm_df = pd.read_csv(os.path.join(args.data_dir, 'sample_submission.csv'))\n",
    "subm_df['time_delta'] = y_pred\n",
    "\n",
    "# Replace negative values (if present) with 1.0\n",
    "subm_df.loc[subm_df['time_delta'] <= 0, 'time_delta'] = 1.0\n",
    "\n",
    "# Save\n",
    "subm_df[['idx', 'time_delta']].to_csv('submission_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a59e82d",
   "metadata": {},
   "source": [
    "### Create calibrated submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a25c6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data groups for calibration\n",
    "test_df['idx'] = test_df['image_id'].map(lambda x: int(x.split('_')[-1]))\n",
    "test_df['species'] = test_df['image'].map(lambda x: x[0].split('/')[5])\n",
    "test_df['ts'] = test_df['image'].map(lambda x: x[0].split('/')[5] + '_' + x[0].split('/')[6])\n",
    "subm_df = pd.merge(subm_df, test_df[['idx', 'species', 'ts']], on='idx', how='left')\n",
    "\n",
    "# Apply calibration coefficients\n",
    "subm_df.loc[subm_df['species'] == 'LT', 'time_delta'] *= 1.10\n",
    "subm_df.loc[subm_df['species'] == 'BC', 'time_delta'] *= 1.05\n",
    "subm_df.loc[(subm_df['ts'] == 'LT_1088') | (subm_df['ts'] == 'LT_1089'), 'time_delta'] *= 0.65\n",
    "\n",
    "# Save\n",
    "subm_df[['idx', 'time_delta']].to_csv('submission_calibrated.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
